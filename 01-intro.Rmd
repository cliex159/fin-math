# (PART) Probability {.unnumbered}

# Distribution

## Discrete

### Slide 11 {.unnumbered}

Flip 2 coins simultaneously, and observe whether a heads or a tail landing up. Define a probability measure for the experiment.

>$$\Omega = \{HH,HT,TH,TT\} $$ 

>$$\begin{align*}
\mathcal{F} = &\{ \emptyset,\Omega,\\
&\{HH\},\{HT,TH,TT\},\\ 
&\{HT\},\{HH,TH,TT\},\\ 
&\{TH\},\{HH,HT,TT\},\\ 
&\{TT\},\{HH,HT,TH\},\\ 
&\{HH,HT\},\{TH,TT\},\\
&\{HH,TH\},\{HT,TT\},\\ 
&\{HH,TT\},\{HT,TH\} \}
\end{align*}$$

>$$\begin{align*}
&\mathcal{P}:\mathcal{F} \rightarrow [0,1]\\
\\
&\mathcal{P}(\emptyset) = 0 \\
&\mathcal{P}(\Omega) = 1 \\
&\mathcal{P}(\{HH\}) = p^2\\
&\mathcal{P}(\{HT,TH,TT\}) = 2p(1-p)+(1-p)^2=1-p^2\\
&\mathcal{P}(\{HT\}) = p(1-p)=p-p^2\\
&\mathcal{P}(\{HH,TH,TT\}) = p^2+p(1-p)+(1-p)^2=p^2-p+1\\
&\mathcal{P}(\{TH\}) = (1-p)p=p-p^2\\
&\mathcal{P}(\{HH,HT,TT\}) = p^2+p(1-p)+(1-p)^2=p^2-p+1\\
&\mathcal{P}(\{TT\}) = (1-p)^2=p^2-2p+1\\
&\mathcal{P}(\{HH,HT,TH\}) = p^2+2p(1-p)=2p-p^2\\
&\mathcal{P}(\{HH,HT\}) = p^2+p(1-p)=p\\
&\mathcal{P}(\{TH,TT\}) = p(1-p) + (1-p)^2=1-p\\
&\mathcal{P}(\{HH,TH\}) = p^2+(1-p)p=p\\
&\mathcal{P}(\{HT,TT\}) = p(1-p)+(1-p)^2=1-p\\
&\mathcal{P}(\{HH,TT\}) = p^2+(1-p)^2=1-2p\\
&\mathcal{P}(\{HT,TH\}) = 2p(1-p)=2p-2p^2\\
\end{align*}$$

### Slide 21 {.unnumbered}

#### Exercise 1

Suppose X is uniformly distributed on {-2, -1, 0, 1, 2}. Find the
probability mass function of $Y = X^2$.
    
>X is uniformly distributed on $\{-2,-1,0,1,2\}$ then
$$Y = X^2 \in \{0,1,4\}$$
and the probability for each number that X is picked is $\frac{1}{5}$

>$$\begin{align*}
&P(Y=0)=P(X^2=1)=P(X=0)=\frac{1}{5} \\ 
&P(Y=1)=P(X^2=1)=P(X=1 \text{ or } X=-1)=\frac{1}{5}+\frac{1}{5}=\frac{2}{5} \\ 
&P(Y=4)=P(X^2=4)=P(X=2 \text{ or } X=-2)=\frac{1}{5}+\frac{1}{5}=\frac{2}{5} 
\end{align*}$$

>Answer: The probability mass function of Y
$$P(Y=K)=\begin{cases}
      \frac{1}{5} & \text{k=0}\\
      \frac{2}{5} & \text{k=1}\\
      \frac{2}{5} & \text{k=4}
    \end{cases} $$
    
#### Exercise 2

A number X is picked uniformly at random from 1 to 100. What is the expected value of $X^2$?
    
>X is uniformly distributed on $\{1,2,...,100\}$ then
$$ Y = X^2 \in \{1,4,...,10000\} $$
and the probability for each number that X is picked is $\frac{1}{100}$

>$$\begin{align*}
&P(Y=1)=P(X^2=1)=P(X=1)=\frac{1}{100} \\ 
&P(Y=4)=P(X^2=4)=P(X=2)=\frac{1}{100} \\ 
&... \\ 
&P(Y=10000)=P(X^2=10000)=P(X=100)=\frac{1}{100}
\end{align*}$$

>The probability mass function of Y
$$P(Y=k)=\begin{cases}
      \frac{1}{100} & \text{k=1}\\
      \frac{1}{100} & \text{k=4}\\
      ...\\
      \frac{1}{100} & \text{k=10000}
    \end{cases} $$
    
>$$E[X^2]=\frac{1}{100} \cdot 1+\frac{1}{100} \cdot 4+...+\frac{1}{100} \cdot 10000=3383.5 $$

>Answer: the expected value of $X^2$ is 3383.5.

#### Exercise 3

Find the variance of X with probability mass function:

$$P(X=k)=\begin{cases}
      \frac{1}{25} & \text{k=1,7}\\
      \frac{3}{25} & \text{k=2,6}\\
      \frac{5}{25} & \text{k=3,5}\\
      \frac{6}{25} & \text{k=4}
    \end{cases} $$

>$$\begin{align*} 
&E[X] = \frac{1}{25} \cdot 1 +
\frac{3}{25} \cdot 2+
\frac{5}{25} \cdot 3+
\frac{7}{25} \cdot 4+
\frac{5}{25} \cdot 5+
\frac{3}{25} \cdot 6+
\frac{1}{25} \cdot 7 = 4 \\
&E[X^2] = \frac{1}{25} \cdot 1^2 +
\frac{3}{25} \cdot 2^2+
\frac{5}{25} \cdot 3^2+
\frac{7}{25} \cdot 4^2+
\frac{5}{25} \cdot 5^2+
\frac{3}{25} \cdot 6^2+
\frac{1}{25} \cdot 7^2 = 18.08 \\
&Var(X) = E[(X-E[X])^2]=E[X^2]-E[X]^2=18.08-16 = 2.08
\end{align*}$$

>Answer: The variance of X is 2.08.

### Slide 26 {.unnumbered}

#### Exercise 1

A single bacterium, name Bobo, lives in a pond. After one minute Bobo will either die, split into 2 bacteria, or stay the same, with equal probability. In subsequent minutes all above living bacteria will independently behave the same way as Bobo. Using the law of total probability, compute the probability that the bacteria population will eventually die out?

>Let p be the probability that the bacteria population will eventually die out. Let X be the bacteria population and Y be the behaviors of the bacterium in subsequent minutes that is stay the same, die, split into 2 bacteria then $Y \in \{0,1,2 \}$ and $$P(Y=0)=P(Y=1)=P(Y=2)=\frac{1}{3}$$

><li>If the bacteria dies out, the probability that the bacteria population will eventually die out is $P(X=0|Y=0)=1$.</li>
<li>If the bacteria remains as it is, the probability that it dies out is $P(X=0|Y=1)=p$.</li>
<li>If the bacteria turns into 2. the probability that it dies out along with its next generation is $P(X=0|Y=2)=p^2$.</li>

>Using the law of total probability, the probability of this bacteria dying out is either itself dies out or the bacterias generated by it also dies out.
$$\begin{align*} 
P(X=0)&= \sum_{y=1}^{3} P(X=0|Y=y)P(Y=y) \\
&=P(X=0|Y=0)P(Y=0) \\
&+P(X=0|Y=1)P(Y=1)+P(X=0|Y=2)P(Y=2) \\
\rightarrow p&=\frac{1}{3}+\frac{1}{3}p+\frac{1}{3}p^2 \\
\rightarrow p&=1
\end{align*} $$

>Answer: The probability that the bacteria population will eventually die out is 1.

#### Exercise 2

A man is trapped in a room with three exits decorated identically at the centre of a maze. Exit 1 leads outside the maze on average after 6 minutes. Exit 2 leads back to the room on average after 9 minutes. Exit 3 leads back to the room on average after 12 minutes. At the room, the man can make an equally likely choice to any of the three exits. What is the expected time taken for the man to leave the maze?

>Let X be the time taken for the man to leave the room and Y be the exit the man chooses on his first step then $Y \in \{1,2,3 \}$ and $P(Y=1)=P(Y=2)=P(Y=3)=\frac{1}{3}$.

><li>Exit 1 leads outside the maze on average after 6 minutes then $E[X|Y=1]=3$.</li>
<li>Exit 2 leads back to the room on average after 9 minutes then $E[X|Y=2]=9+E[X]$.</li>
<li>Exit 3 leads back to the room on average after 12 minutes then $E[X|Y=3]=12+E[X]$.</li>


>Using the law of total expectation, the expected time taken for the man to leave the maze is the expected time the man have to take to make an equally likely choice to any of the three exits.
$$\begin{align*} 
E[X]&=E_Y\{E[X|Y]\} \\
&= \sum_{y=1}^{3} E[X|Y=y]P(Y=y) \\
&=E[X|Y=1]P(Y=1)+E[X|Y=2]P(Y=2)+E[X|Y=3]P(Y=3) \\
&=\frac{1}{3}(3)+\frac{1}{3}(9+E[X])+\frac{1}{3}(12+E[X]) \\
\rightarrow E[X]&=24
\end{align*}
$$

>Answer: The expected time taken for the man to leave the maze is 24 minutes.

### Slide 32 {.unnumbered}

Angel will daily harvest a random number N tomatoes in her garden, where N has a Poisson distribution with parameter 1000 per day. Each tomato is checked for defects. The chance that a tomato has defects is 10%. Defects are independent from tomato to tomato. Find the expected number of tomatoes with defects.

>Let X be the number of tomatoes with defects.

>N has a Poisson distribution with parameter 1000 per day so the expected number of tomatoes is 1000: 
$$N \sim P(1000) \rightarrow E[N]=1000$$

>The chance that a tomato has defects is 10% then the probability of tomatoes with defects out of n tomatoes follows binomial distribution with $p=0.1$: 
$$X \sim B(0.1,n) \rightarrow E[X]=0.1n$$
Let the number of n tomatoes be E[N] we have
$$E[X]=0.1E[N]=0.1 \cdot 1000=100$$

>Answer: The expected number of tomatoes with defects is 100


### Slide 24 {.unnumbered}

A fair coin is flipped repeatedly. Find the expected number of flips needed to get two heads in a row.

> Let X be the expected number of flips we need to get two heads in a row. 
Flipping 2 coins repeatedly and observe two consecutive outcomes, we have the following cases:

><p align="center">
  <img src="https://drive.google.com/uc?export=view&id=1uGWB38oS8ryrlLdp1_qqKyxz4CHcMMgt">
</p>  

><h4>Case 1 (T): A tail appears on the first flip, we do not need to flip the second one.  </h4>
On the first flip of the coin, there is a 1/2 chance we get a tails. In this case, we are back at the starting point and we have wasted 1 flip thus the expected number of flips needed will be X + 1.

><h4>Case 2 (HT): A heads appears on the first flip and then a tails appears on the second flip.  </h4>
The chance a heads appears on the first flip is $\frac{1}{2}$ and the chance a tails appears on the second flip is $\frac{1}{2}$ so the probability to get a heads appears on the first flip and then a tails appears on the second flip is $\frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}$. In this case, we are back at the starting point and we have wasted 2 flips thus the expected number of flips needed will be X + 2.

><h4>Case 3 (HH): Two heads appear on two tosses.  </h4>
The chance a heads appears on the first flip is $\frac{1}{2}$ and the chance a heads appears on the second flip is $\frac{1}{2}$ so the probability to get two heads appear on two flips is $\frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}$. In this case, the expected flips needed will be 2.

> Framing the above three cases in the form of equations and adding we will get:
$$\begin{align*}
X&=(1+X)\frac{1}{2}+(2+X)\frac{1}{4}+(2) \frac{1}{4} \\
    \rightarrow X&=6
    \end{align*}$$

> Answer: On average, the coin needs flipping 6 times to get two heads in a row.

### Slide 25  {.unnumbered}

#### Exercise 1

A fair coin is flipped repeatedly. Find the expected number of flips
needed to get three heads in a row.  

> Let X be the expected number of flips we need to get two three in a row. 
Flipping 3 coins repeatedly and observe three consecutive outcomes, we have the following cases:

><p align="center">
  <img src="https://drive.google.com/uc?export=view&id=1TqkVrV6orUEHAcWc4ObVFYl_i5218VPD">
</p>

><h4>Case 1 (T): A tail appears on the first flip, we do not need to flip the second and third one.  </h4>
On the first flip of the coin, there is a 1/2 chance we get a tails. In this case, we are back at the starting point and we have wasted 1 flip thus the expected number of flips needed will be X + 1.

><h4>Case 2 (HT): A heads appears on the first flip and then a tails appears on the second flip, we do not need to flip the third one.  </h4>
The chance a heads appears on the first flip is $\frac{1}{2}$ and the chance a tails appears on the second flip is $\frac{1}{2}$ so the probability to get a heads appears on the first flip and then a tails appears on the second flip is $\frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}$. In this case, we are back at the starting point and we have wasted 2 flips thus the expected number of flips needed will be X + 2.

><h4>Case 3 (HHH): Three heads appear on three flips.  </h4>
The chance a heads appears on the first flip is $\frac{1}{2}$ and the chance a heads appears on the second flip is $\frac{1}{2}$ and the chance a heads appears on the third flip is $\frac{1}{2}$ so the probability to get three heads appear on three flips heads appear on two flips is $\frac{1}{2} \cdot \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{8}$. In this case, the expected flips needed will be 2.

><h4>Case 4 (HHT): Two heads appear on two flips and then a tails appears on the third flip.  </h4>
The chance a heads appears on the first flip is $\frac{1}{2}$ and the chance a heads appears on the second flip is $\frac{1}{2}$ and the chance a tails appears on the third flip is $\frac{1}{2}$ so the probability to get two heads appear on two flips and then a tails appears on the third flip is $\frac{1}{2} \cdot \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{8}$. In this case, we are back at the starting point and we have wasted 3 flips thus the expected number of flips needed will be X + 3.

> Framing the above three cases in the form of equations and adding we will get:
> Framing the above three cases in the form of equations and adding we will get:
$$\begin{align*}
    X&=(1+X)\frac{1}{2}+(2+X)\frac{1}{4}+(3)\frac{1}{8}+ (3+X)\frac{1}{8} \\
        \rightarrow X&=14
        \end{align*}$$

> Answer: On average, the coin needs flipping 6 times to get two heads in a row.

#### Exercise 2

A fair coin is flipped repeatedly. Find the expected number of flips
needed to get 2 consecutive heads followed by a tails (HHT in a row).

> Let X be the expected number of flips we need to get two three in a row. 
Flipping 3 coins repeatedly and observe three consecutive outcomes, we have the following cases:

><p align="center">
  <img src="https://drive.google.com/uc?export=view&id=1AD95VJziqQZMsMe217sEwy9qTKu3EDkE">
</p>

><h4>Case 1 (T): A tail appears on the first flip, we do not need to flip the second and third one.  </h4>
On the first flip of the coin, there is a 1/2 chance we get a tails. In this case, we are back at the starting point and we have wasted 1 flip thus the expected number of flips needed will be X + 1.

><h4>Case 2 (HT): A heads appears on the first flip and then a tails appears on the second flip, we do not need to flip the third one.  </h4>
The chance a heads appears on the first flip is $\frac{1}{2}$ and the chance a tails appears on the second flip is $\frac{1}{2}$ so the probability to get a heads appears on the first flip and then a tails appears on the second flip is $\frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}$. In this case, we are back at the starting point and we have wasted 2 flips thus the expected number of flips needed will be X + 2.

><h4>Case 3 (HHH): Three heads appear on three flips.  </h4>
The chance a heads appears on the first flip is $\frac{1}{2}$ and the chance a heads appears on the second flip is $\frac{1}{2}$ and the chance a heads appears on the third flip is $\frac{1}{2}$ so the probability to get three heads appear on three flips heads appear on two flips is $\frac{1}{2} \cdot \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{8}$. In this case, we are back at the starting point and we have wasted 3 flips thus the expected number of flips needed will be X + 3.

><h4>Case 4 (HHT): Two heads appear on two flips and then a tails appears on the third flip.  </h4>
The chance a heads appears on the first flip is $\frac{1}{2}$ and the chance a heads appears on the second flip is $\frac{1}{2}$ and the chance a tails appears on the third flip is $\frac{1}{2}$ so the probability to get two heads appear on two flips and then a tails appears on the third flip is $\frac{1}{2} \cdot \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{8}$. In this case, the expected flips needed will be 2.

> Framing the above three cases in the form of equations and adding we will get:
$$\begin{align*}
X&=(1+X)\frac{1}{2}+(2+X)\frac{1}{4}+(3+X)\frac{1}{8}+(3) \frac{1}{8} \\
    \rightarrow X&=14
    \end{align*}$$

> Answer: On average, the coin needs flipping 14 times to get two consecutive heads followed by a tails (HHT in a row).

#### Exercise 3

You play a slot machine repeatedly. The probability of winning on a
single play is 5%, and successive plays are independent. The random
variable of interest is X, the number of unsuccessful attempts before
the first win.

a) Find an expression for the cumulative distribution function of X.

>X is the number of unsuccessful attempts before the first win so X follows the Geometric distribution having the cumulative distribution:
$$\begin{align*}
P(X \leq k) &=1-(1-p)^k \\ 
&=1-(1-0.05)^x \\ 
&=1-0.95^x 
\end{align*}$$

>Answer: The expression for the cumulative distribution function of X: $P(X \leq k)=1-0.95^x$.

b) How many times would you need to play the slot machine in order
to be sure that your probability of winning at least once is greater
than or equal to 90%?

>In order to be sure that the probability of winning at least once is greater than or equal to 90%:
$$\begin{align*}
&P(X \leq k) &\geq 0.9 \\ 
&\rightarrow 1-0.95^x &\geq 0.9 \\ 
&\rightarrow x &\leq 44.89 
\end{align*}$$

>Answer: I need to play 45 times in order to be sure that the probability of winning at least one is greater than or equal to 90%.

## Continuous

### Slide 33 {.unnumbered}

When Katie goes to the gym, she will either run, bicycle, or row. She will
choose one of the aerobic activities with respective probabilities 0.5, 0.3,
and 0.2. And having chosen an activity, the amount of time (in minutes)
she spends exercising is exponentially distributed with respective
parameters 0.05, 0.025, and 0.01. Find the expectation of Katie exercise
time.

>Let X be the Katie exercise time and Y be a random variable that takes values 1, 2, and 3 corresponding to her choice of running, bicycling, and rowing.

>The amount of time (in minutes)
she spends running is exponentially distributed with respective
parameters 0.05 so the expectation of Katie run time is $\frac{1}{0.05}=20$ (minutes).

>The amount of time (in minutes)
she spends riding bicycle is exponentially distributed with respective
parameters 0.025 so the expectation of Katie bicycle time is $\frac{1}{0.025}=40$ (minutes).

>The amount of time (in minutes)
she spends rowing is exponentially distributed with respective
parameters 0.01 so the expectation of Katie row time is $\frac{1}{0.01}=100$ (minutes).

>The expectation of Katie exercise time for each activity is 20,40 and 100 minutes respectively. The mass function of X is:
$$\begin{align*}
P(Y=k)=\begin{cases}
      0.5 & \text{k=1}\\
      0.3 & \text{k=2}\\
      0.2 & \text{k=3}
    \end{cases}
\end{align*}$$

>Using the law of total expectation, the expectation of Katie exercise time is the expected time for her choice of running, bicycling, and rowing.
$$\begin{align*}
E[X]&=E[X|Y=1]P(Y=1)+E[X|Y=2]P(Y=2)+E[X|Y=3]P(Y=3) \\
&=0.5 \cdot 20+ 0.3 \cdot 40+0.2 \cdot 100 \\
&=42
\end{align*}$$

>Answer: The expectation of Katie exercise time is 42 minutes.


### Slide 35   {.unnumbered}

Assume the length of withdrawing money in minutes in an ATM booth is an exponential random variable with rate $\lambda = 0.5$.  
If someone arrives at the ATM booth before you arrive, find the probability that someone have to wait:

>Let X be the length of withdrawing money in minutes in an ATM booth. X is an exponential random variable with rate $\lambda = 0.5$ then the cumulative distribution function:
$$\begin{align*} 
P(X<x) &=1-e^{- \lambda x} \\
&=1-e^{- 0.5 x}
\end{align*}$$

- less than 1 minute  

>The probability that someone have to wait less than 1 minute:
$$\begin{align*} 
P(X<1)&=1-e^{- 0.5 \cdot 1} \\
&=0.3935
\end{align*}$$

>Answer: The probability that someone have to wait less than 1 minute is 39.35\%

- greater than 3 minutes  

>The probability that someone have to wait greater than 3 minutes:
$$\begin{align*}
P(X>3)&=1-P(X>3) \\
&=1-(1-e^{- 0.5 \cdot 3}) \\
&=0.2231
\end{align*}$$

>Answer: The probability that someone have to wait greater than 3 minutes is 22.31\%

- between 1 to 3 minutes  

>The probability that someone have to wait between 1 to 3 minutes:
$$\begin{align*}
P(1<X<3) &= P(X<3)-P(X<1) \\
&=1-e^{- 0.5 \cdot 3}-(1-e^{- 0.5 \cdot 1}) \\
&=e^{- 0.5 \cdot 1}-e^{- 0.5 \cdot 3} \\
&=0.3834
\end{align*}$$

>Answer: The probability that someone have to wait between 1 to 3 minutes is 38.34\%  

### Slide 37   {.unnumbered}

When Dung goes to the gym, he will either run, bicycle, or row. He will choose one of the aerobic activities with respective probabilities 0.5, 0.3, and 0.2. If he chooses running or bicycling, the amount of time (in minutes) he spends exercising is exponentially distributed with respective parameters 0.05, 0.025. Whenever he chooses rowing he only rows for 10 minutes, stops to take a drink of water, and starts all over, choosing one of the three activities at random as if he had just walked in the door.  
Find the expectation of Dungs exercise time.  

>Let X be the his exercise time and Y be a random variable that takes values 1, 2, and 3 corresponding to his choice of running, bicycling, and rowing.

>The amount of time (in minutes)
he spends running is exponentially distributed with respective
parameters 0.05 so the expectation of Katie run time is $\frac{1}{0.05}=20$ (minutes).

>The amount of time (in minutes)
he spends riding bicycle is exponentially distributed with respective
parameters 0.025 so the expectation of Katie bicycle time is $\frac{1}{0.025}=40$ (minutes).

>The amount of time (in minutes)
he spends rowing is 10 minutes, stops to take a drink of water, and starts all over so the expectation of Katie row time is $10+E[X]$ (minutes).

>The expectation of his exercise time for each activity is 20,40 and 10+E[X] minutes respectively. The mass function of X is:
$$\begin{align*}
P(X=k)&=\begin{cases}
      0.5 & \text{k=1}\\
      0.3 & \text{k=2}\\
      0.2 & \text{k=3}
    \end{cases} \\
\end{align*} $$

>Using the law of total expectation, the expectation of his exercise time is the expected time for her choice of running, bicycling, and rowing.
$$\begin{align*}
E[X]&= E[X|Y=1]P(Y=1)+E[X|Y=2]P(Y=2)+E[X|Y=3]P(Y=3) \\
&= 0.5 \cdot 20+0.3 \cdot 40+0.2 \cdot (10+E[X]) \\ 
\rightarrow E[X]&=30 
\end{align*}$$

>Answer: The expectation of Dung exercise time is 30 minutes.

### Slide 69 {.unnumbered}

Let X be a continuous normal random variable with the probability density
function $f(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}}$ 

1. Verify $\int_{-\infty}^{\infty} f(x) \,dx=1$

>$$\begin{align*}
\int_{-\infty}^{\infty} \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}}\,dx 
&=\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}}\,d \left( \frac{x-\mu}{\sigma} \right) \\
&=\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}y^2}\,d y \\
&=I
\end{align*}$$

>$$\begin{align*}
I^2&=\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}y^2}\,d y \cdot \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}y^2}\,d y \\
&= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}x^2}\,d x \cdot \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}y^2}\,d y \\
&=\frac{1}{{2\pi}}\int_{-\infty}^{\infty} \int_{-\infty}^{\infty}e^{-\frac{1}{2}(x^2+y^2)}\,d x\,d y \\
&=\frac{1}{{2\pi}}\int_{0}^{\infty} \int_{0}^{2\pi} r e^{\frac{-r^2}{2}}\,d r \,d \theta \\
&=\frac{1}{{2\pi}} \cdot -2\pi e^{\frac{-r^2}{2}}\Biggr|_{0}^{\infty}=1
\end{align*}$$

2. Compute the moment generating function $E[e^{tX}]$ and then derive $E[X^4]$.

>Let $u=\frac{x-\mu}{\sqrt{2\sigma}}$, $v=u-\frac{\sqrt{2}}{2} \sigma t$ and  $I=\int_{-\infty}^{\infty} e^{-v^2}\,dv$
$$\begin{align*}
I^2&=\int_{-\infty}^{\infty}  e^{-v^2}\,dy \cdot \int_{-\infty}^{\infty}  e^{-v^2}\,dv \\
&= \int_{-\infty}^{\infty}  e^{-x^2}\,d x \cdot \int_{-\infty}^{\infty}  e^{-y^2}\,dy \\
&=\int_{-\infty}^{\infty} \int_{-\infty}^{\infty}e^{-(x^2+y^2)}\,dx\,dy \\
&=\int_{0}^{\infty} \int_{0}^{2\pi} r e^{-r^2}\,dr \,d\theta \\
&= -2\pi \cdot \frac{1}{2} e^{-r^2}\Biggr|_{0}^{\infty}=\pi
\end{align*}$$

>$$\begin{align*}
E[e^{tX}]&= \int_{-\infty}^{\infty} e^{tx} \cdot \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}} \,dx \\
&=\frac{1}{\sigma \sqrt{2\pi}} \int_{-\infty}^{\infty} e^{tx-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}} \,dx \\
&=\frac{\sqrt{2}\sigma}{\sigma \sqrt{2\pi}} \int_{-\infty}^{\infty} e^{t(\mu+\sqrt{2}\sigma u)-u^2} \,du \\
&=\frac{e^{\mu t}}{ \sqrt{\pi}} \int_{-\infty}^{\infty} e^{-\left(u-\frac{\sqrt{2}}{2}\sigma t \right)^2+\frac{1}{2}\sigma^2 t^2} \,du \\
&=\frac{e^{\mu t+ \frac{1}{2}\sigma^2 t^2}}{ \sqrt{\pi}} \int_{-\infty}^{\infty} e^{-v^2}\,dv=\frac{e^{\mu t+ \frac{1}{2}\sigma^2 t^2}}{ \sqrt{\pi}} \cdot I \\
&=e^{\mu t+ \frac{1}{2}\sigma^2 t^2}=M(t)
\end{align*}$$

>$$\begin{align*}
M'(t)&=(\mu+\sigma^2t)e^{\mu t+ \frac{1}{2}\sigma^2 t^2} \\
M''(t)&=[(\mu+\sigma^2t)^2+\sigma^2]e^{\mu t+ \frac{1}{2}\sigma^2 t^2} \\
M^{(3)}(t)&=[(\mu+\sigma^2t)^3+3\sigma^2(\mu+\sigma^2t)]e^{\mu t+ \frac{1}{2}\sigma^2 t^2} \\
M^{(4)}(t)&=[(\mu+\sigma^2t)^4 +6\sigma^2(\mu+\sigma^2t)^2+3\sigma^4]e^{\mu t+ \frac{1}{2}\sigma^2 t^2} \\
\end{align*}$$

>$$\begin{align*}
E[X^4]&= M^{(4)}(0) \\
&=\mu^4 +6\mu^2\sigma^2+3\sigma^4
\end{align*}$$  

>Answer: The moment generating function is $E[e^{tX}]=e^{\mu t+\frac{1}{2}\sigma^2t^2}$ and the derivative of $E[X^4]=\mu^4 +6\mu^2\sigma^2+3\sigma^4$  

3. Compute the mean and variance of $X$.

>$$\begin{align*}
E[X]&= M'(0)=\mu \\ 
E[X^2]&=M''(0)=\mu^2+\sigma^2 \\ 
Var(X)&=E[X^2]-E[X]^2 \\
&=\sigma^2
\end{align*}$$  

>Answer: The mean, and variance of $X$ is $E[X]=\mu$, and $Var(X)=\sigma^2$, respectively  

4. Compute the mean and variance of $e^X$.

>$$\begin{align*}
E[e^X]&=e^{\mu+ \frac{1}{2}\sigma^2} \\ 
E[(e^X)^2]&=E[e^{2X}]=e^{2\mu+ 2\sigma^2} \\ 
Var(e^X)&=E[(e^X)^2]-E[e^X]^2 \\
&=e^{2\mu+ 2\sigma^2}-e^{2\mu+ \sigma^2} \\
&=e^{2\mu+ \sigma^2}(e^{\sigma^2}-1)
\end{align*}$$  

>Answer: The mean of $e^X$ is $E[e^X]=e^{\mu+\frac{1}{2}\sigma^2}$ and its variance is $Var(e^X)=e^{2\mu+ \sigma^2}(e^{\sigma^2}-1)$  

### Slide 73 {.unnumbered}

Let X be a continuous uniform random variable with the probability density function 
$$f(x)= \frac{1}{b-a}, \forall a \leq x \leq b$$

1. Verify $\int_{a}^{b} f(x) \,dx=1$

>$$\begin{align*}
\int_{a}^{b} \frac{1}{b-a} \,dx &=\frac{x}{b-a}\Biggr|_{a}^{b} \\ 
&=\frac{b}{b-a}-\frac{a}{b-a} \\
&=1
\end{align*}$$

2. Compute the mean and variance of $X$.

>$$\begin{align*}
E[X]&=\int xf(x) \,dx \\
&=\int_{a}^{b} x \cdot \frac{1}{b-a} \,dx \\
&=\frac{x^2}{2(b-a)}\Biggr|_{a}^{b} \\
&=\frac{b^2}{2(b-a)}-\frac{a^2}{2(b-a)}  \\
&=\frac{a+b}{2}
\end{align*}$$

>$$\begin{align*} 
Var(X)&=E[X^2]-E[X]^2 \\
&=\int_{a}^{b} \left(\frac{x^2}{b-a}\right)- \left(\frac{a+b}{2} \right)^2 \,dx \\
&=\frac{x^3}{3(b-a)}\Biggr|_{a}^{b} - \left(\frac{a+b}{2} \right)^2 \\
&=\frac{b^3-a^3}{3(b-a)}-\frac{a^2+2ab+b^2}{4} \\
&=\frac{(b-a)^2}{12}
\end{align*}$$  

>Answer: The mean of $X$ is $E[X]=\frac{a+b}{2}$ and its variance is $Var(X)=\frac{(b-a)^2}{12}$  

3. Compute the cumulative distribution function of $X$.
  
>$$\begin{align*}
F(x)&=P(X \leq x) \\
&=\int_{a}^{x}f(t)\,dt \\ 
&=\int_{a}^{x} \frac{1}{b-a}\,dt \\ 
&= \frac{t}{b-a}\Biggr|_{a}^{x} \\
&=\frac{x-a}{b-a} 
\end{align*}$$  

>Answer: The cumulative distribution function of X is:  
$$ F(x) =
  \begin{cases}
     1      & \quad \text{if } x \geq b \\
     \frac{x-a}{b-a}  & \quad \text{if } a < x < b \\
     0    & \quad \text{if } x \leq a
  \end{cases}$$  

### Slide 74 {.unnumbered}

Let X be a exponential random variable with the probability density function 
$$f(x)= \lambda e^{-\lambda x}, x>0,\lambda >0$$

1. Verify $\int_{0}^{+\infty} f(x) \,dx=1$

>$$\begin{align*}
\int_{0}^{\infty} \lambda e^{-\lambda x} \,dx &= -\lambda \cdot \frac{e^{-\lambda x}}{\lambda}\Biggr|_{0}^{\infty} \\ 
&=-e^{-\lambda x}\Biggr|_{0}^{\infty} \\
&=1
\end{align*}$$

2. Compute the mean and variance of $X$.

>$$\begin{align*}
E[e^{tX}]&= \int_{0}^{\infty} f(x)e^{tx} \,dx \\
&=\int_{0}^{\infty} \lambda e^{-\lambda x}e^{tx} \,dx \\
&=\lambda\int_{0}^{\infty}e^{x(t-\lambda)}\,dx \\
&= \lambda \frac{-e^{x(t-\lambda)}}{t-\lambda}\Biggr|_{0}^{\infty} \\
&=\frac{\lambda}{\lambda-t}  \\
&=M(t)
\end{align*}$$

>$$\begin{align*}
M'(t)&=E[Xe^{tX}] &&=\frac{-\lambda}{(\lambda-t)^2} \\
\rightarrow E[X]&=M'(0)&&=\frac{-1}{\lambda} \\
M''(t)&=E[X^2e^{tX}]&&=\frac{2\lambda}{(\lambda-t)^3} \\
\rightarrow E[X^2]&=M''(0)&&=\frac{2}{\lambda^2} \\
\rightarrow Var(X)&=E[X^2]-E[X]^2&&=\frac{2}{\lambda^2}-\left(\frac{-1}{\lambda} \right)^2=\frac{1}{\lambda^2}&&
\end{align*}$$  

>Answer: The mean of $X$ is $E[X]=\frac{-1}{\lambda}$ and its variance is $Var(X)=\frac{1}{\lambda^2}$  

3. Compute the cumulative distribution function of $X$.

>$$\begin{align*}
F(x)&=P(X \leq x) \\
&=\int_{0}^{x}f(t)\,dt \\ 
&=\int_{0}^{x}\lambda e^{-\lambda t}  \,dt \\ 
&= -e^{-\lambda t}\Biggr|_{0}^{x} \\
&=1-e^{-\lambda x}
\end{align*}$$  

>Answer: The cumulative distribution function of $X$ is:  
$$ F(x) =
  \begin{cases}
     1-e^{-\lambda x}      & \quad \text{if } x > 0\\
     0  & \quad \text{if } x \leq 0
  \end{cases}$$

### Slide 75 {.unnumbered}

Let X be a continuous log-normal random variable with the probability density function 
$$f(x)= \frac{1}{x\sigma\sqrt{2\pi}}e^{-\frac{(\ln x - \mu)^2}{2 \sigma^2}}$$

1. Verify $\int_{-\infty}^{+\infty} f(x) \,dx=1$

>$$\begin{align*}
\int_{-\infty}^{+\infty}\frac{1}{x\sigma\sqrt{2\pi}}e^{-\frac{(\ln x - \mu)^2}{2 \sigma^2}}\,dx &=\int_{-\infty}^{+\infty}\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(\ln x - \mu)^2}{2 \sigma^2}}\,d(\ln x) \\ &=\int_{-\infty}^{+\infty}\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y - \mu)^2}{2 \sigma^2}}\,dy=1
\end{align*}$$

2. Compute the mean and variance of $X$.

>$$\begin{align*}
E[e^X]&=e^{\mu+ \frac{1}{2}\sigma^2} \\ 
E[(e^X)^2]&=E[e^{2X}]=e^{2\mu+ 2\sigma^2} \\ 
Var(e^X)&=E[(e^X)^2]-E[e^X]^2 \\
&=e^{2\mu+ 2\sigma^2}-e^{2\mu+ \sigma^2} \\
&=e^{2\mu+ \sigma^2}(e^{\sigma^2}-1)
\end{align*}$$

>$$\begin{align*}
E[X]=E[e^Y]&=e^{\mu+ \frac{1}{2}\sigma^2} \\ 
E[(e^Y)^2]&=E[e^{2Y}]=e^{2\mu+ 2\sigma^2} \\ 
Var(X)=Var(e^Y)&=E[(e^Y)^2]-E[e^Y]^2 \\
&=e^{2\mu+ 2\sigma^2}-e^{2\mu+ \sigma^2} \\
&=e^{2\mu+ \sigma^2}(e^{\sigma^2}-1)
\end{align*}$$  

>Answer: The mean of X is $E[X]=e^{\mu+\frac{1}{2}\sigma^2}$ and its variance is $Var(X)=e^{2\mu+ \sigma^2}(e^{\sigma^2}-1)$  

# Stochastic Process

## Random Walk

### Slide 28 {.unnumbered}

A gambler starts with \$3 dollars. On each play a fair coin is tossed and the gambler wins \$1 if heads occurs, or loses \$1 if tails occurs. The gambler stops when he reaches \$8 or loses all his money. Find the probability that the gambler will eventually lose.

>Let $R_n$ denote the total fortune after the $n^{th}$ gamble.
While the game proceeds, $\{R_n : n \geq 0\}$ forms a simple random walk:
$$R_n = R_0+ \Delta_1 +\Delta_2+ · · · + \Delta_n $$
where $\{\Delta n\}$ forms an i.i.d. random variables as $P(\Delta = 1) = p$, $P(\Delta = −1) = q$, $p+q=1$, and represents the earnings on the succesive gambles.


>Let $P_3$ denote the probability that the gambler wins when $R_0 = 3$, we need to find $1-P_3$.
<ul>
<li>If $\Delta_1 = 1$, then the gambler’s total fortune
increases to $R_1 = 3+1$ and so the gambler will now win with probability $P_{4}$.</li>
<li>If $\Delta_1 = −1$, then the gambler’s fortune decreases to $R_1 = 3 − 1$ and so the gambler will now win with probability $P_{2}$.</li>
</ul>
$$\begin{align*}
P_3&= pP_{4}+qP_{2} \\ 
pP_{3}+qP_{3}&= pP_{4}+qP_{2} \\ 
p(P_4-P_3)&=q(P_3-P_2) \\ 
P_4-P_3&=\frac{q}{p}(P_3-P_2) \\ 
&=\left( \frac{q}{p} \right)^2(P_2-P_1) \\
&=\left( \frac{q}{p} \right)^3 (P_1-P_0) \\
&=\left( \frac{q}{p} \right)^3 (P_1)
\end{align*}$$

>By induction, we can prove $P_{k+1}-P_k= \left( \frac{q}{p} \right)^k P_1$, which implies the following recursive sequence:
$$\begin{align*}
P_2-P_1&=\left( \frac{q}{p} \right)(P_1-P_0)=\left( \frac{q}{p} \right) P_1\\
P_3-P_2&=\left( \frac{q}{p} \right)(P_2-P_1)=\left( \frac{q}{p} \right)^2 P_1 \\
...\\
P_{i+1}-P_i&= \left( \frac{q}{p} \right)^i P_1
\end{align*}$$

>Adding all the above sequence and noticing that $P_N=1$ shall yield:

>$$\begin{align*}
P_{i+1}-P_1&=  \sum_{k=1}^{i} (P_{k+1}-P_k) \\
&=\sum_{k=1}^{i} \left( \frac{q}{p} \right)^k P_1 \\ 
\end{align*}$$

>
$$\begin{align*}
>P_{i+1}&=P_1+\sum_{k=1}^{i} \left( \frac{q}{p} \right)^k P_1 \\ 
&= P_1\sum_{k=0}^{i} \left( \frac{q}{p} \right)^k \\ 
&=P_1\sum_{k=0}^{i} \left( \frac{0.5}{0.5} \right)^k  \\
&=P_1\sum_{k=0}^{i} \left( 1 \right)^k  \\
&=P_1(i+1) \\
\rightarrow P_N&=P_1\cdot N \\
\rightarrow P_1&=\frac{1}{N} \\
\rightarrow P_3&=3 P_1 \\
&=3\cdot \frac{1}{N} \\ 
&=\frac{3}{N} \\
&=\frac{3}{8} \\
\rightarrow 1-P_3&=\frac{5}{8} \\
\end{align*}$$

>Answer: The probability that the gambler will eventually lose is 62.5\%.

### Slide 29 {.unnumbered}

A gambler starts with $3 dollars. On each play an unfair coin is tossed and
the gambler wins \$1 if heads occurs, with probability 0.6, or loses \$1 if
tails occurs, with probability 0.4. The gambler stops when he reaches $8
or loses all his money. Find the probability that the gambler will eventually
lose.

>Let $R_n$ denote the total fortune after the $n^{th}$ gamble.
While the game proceeds, $\{R_n : n \geq 0\}$ forms a simple random walk:
$$R_n = R_0+ \Delta_1 +\Delta_2+ · · · + \Delta_n $$
where $\{\Delta n\}$ forms an i.i.d. random variables as $P(\Delta = 1) = p$, $P(\Delta = −1) = q$, $p+q=1$, and represents the earnings on the succesive gambles.


>Let $P_3$ denote the probability that the gambler wins when $R_0 = 3$, we need to find $1-P_3$.
<ul>
<li>If $\Delta_1 = 1$, then the gambler’s total fortune
increases to $R_1 = 3+1$ and so the gambler will now win with probability $P_{4}$.</li>
<li>If $\Delta_1 = −1$, then the gambler’s fortune decreases to $R_1 = 3 − 1$ and so the gambler will now win with probability $P_{2}$.</li>
</ul>
$$\begin{align*}
P_3&= pP_{4}+qP_{2} \\ 
pP_{3}+qP_{3}&= pP_{4}+qP_{2} \\ 
p(P_4-P_3)&=q(P_3-P_2) \\ 
P_4-P_3&=\frac{q}{p}(P_3-P_2) \\ 
&=\left( \frac{q}{p} \right)^2(P_2-P_1) \\
&=\left( \frac{q}{p} \right)^3 (P_1-P_0) \\
&=\left( \frac{q}{p} \right)^3 (P_1)
\end{align*}$$

>By induction, we can prove $P_{k+1}-P_k= \left( \frac{q}{p} \right)^k P_1$, which implies the following recursive sequence:
$$\begin{align*}
P_2-P_1&=\left( \frac{q}{p} \right)(P_1-P_0)=\left( \frac{q}{p} \right) P_1\\
P_3-P_2&=\left( \frac{q}{p} \right)(P_2-P_1)=\left( \frac{q}{p} \right)^2 P_1 \\
...\\
P_{i+1}-P_i&= \left( \frac{q}{p} \right)^i P_1
\end{align*}$$

>Adding all the above sequence and noticing that $P_N=1$ shall yield:

>$$\begin{align*}
P_{i+1}-P_1&=  \sum_{k=1}^{i} (P_{k+1}-P_k) \\
&=\sum_{k=1}^{i} \left( \frac{q}{p} \right)^k P_1 \\ 
\end{align*}$$

>$$\begin{align*}
>P_{i+1}&=P_1+\sum_{k=1}^{i} \left( \frac{q}{p} \right)^k P_1 \\ 
&= P_1\sum_{k=0}^{i} \left( \frac{q}{p} \right)^k \\ 
&=P_1 \cdot \frac{1-(\frac{q}{p})^{i+1}}{1-\frac{q}{p}} &(1) \\
\end{align*}$$

>$$\begin{align*}
P_N&=P_1 \cdot \frac{1-(\frac{q}{p})^N}{1-\frac{q}{p}} \\
P_1&= \frac{1-\frac{q}{p}}{1-(\frac{q}{p})^N}&(2) \\
\end{align*}$$

>$$\begin{align*}
(1),(2)\rightarrow P_{i+1}&=\frac{1-(\frac{q}{p})^{i+1}}{1-(\frac{q}{p})^N} \\ 
P_{3}&=\frac{1-(\frac{q}{p})^{3}}{1-(\frac{q}{p})^8} \\
1-P_{3}&=1-\frac{1-(\frac{q}{p})^{3}}{1-(\frac{q}{p})^8}=0.2677
\end{align*}$$

>Answer: The probability that the gambler will eventually lose is 26.77\%.

### Slide 45   {.unnumbered}

For each toss of a fair coin, you will win $\$1$ if it lands up heads and lose $\$1$ if it lands up tails. Suppose that you start the game with $\$0$ and denote $M_k$ is the amount of money you obtain after k tosses. Then $(M_k)_{k\in \mathbb{N}}$ is a random walk.  

- Play the game with your friend 20 times and plot your accumulated money after every play.

>While the game proceeds, $(M_k)_{k\in \mathbb{N}}$ forms a simple random walk:
$$M_k = M_0+ \Delta_1 +\Delta_2+ · · · + \Delta_k $$
where $M_0=0$, $\{\Delta n\}$ forms an i.i.d. random variables as $P(\Delta = 1) = \frac{1}{2}$, $P(\Delta = −1) = \frac{1}{2}$, and represents the earnings on the succesive games.

>Using the law of total expectation, the expected accumulated money after every play is the expected money for each toss either the coin lands up head or tail.
$$\begin{align*} 
E[\Delta] &= E[\Delta|\Delta=1]P(\Delta=1)+E[\Delta|\Delta=-1]P(\Delta=-1) \\
&=\frac{1}{2}(1)+\frac{1}{2}(-1) \\
&=0
\end{align*}
$$

>Answer: The expected accumulated money after every play is 0\$.

- Write a R code to simulate the game  

```{r}
library(tidyverse)
par(mfrow=c(3,3))
plots=map(1:9,
          ~cumsum(x=sample(c(1,-1), # random walk
                  size=20, 
                  replace=T,
                  prob=c(0.5,0.5)))) %>% 
      map(
          plot,         # plot
          type="o", 
          col="blue",
          xlab="number of playing games",
          ylab="the accumulated money")
```

### Slide 48   {.unnumbered}

For each toss of a unfair coin, you will win $\$2$ if it lands up heads and lose $\$1$ if it lands up tails. The probability of landing up heads is 0.3 and the probability of landing up tails is 0.7. Suppose that you start the game with $\$0$ and denote $M_k$ is the amount of money you obtain after k tosses.  

- Should you play the game?  

>While the game proceeds, $(M_k)_{k\in \mathbb{N}}$ forms a simple random walk:
$$M_k = M_0+ \Delta_1 +\Delta_2+ · · · + \Delta_k $$
where $M_0=0$, $\{\Delta n\}$ forms an i.i.d. random variables as $P(\Delta = 2) = 0.3$, $P(\Delta = −1) = 0.7$, and represents the earnings on the succesive games.

>Using the law of total expectation, the expected accumulated money after every play is the expected money for each toss either the coin lands up head or tail.
\begin{align*} 
E[\Delta] &=E[\Delta|\Delta=2]P(\Delta=2)+E[\Delta|\Delta=-1]P(\Delta=-1) \\
&=0.3(2)+0.7(-1) \\
&=-0.1
\end{align*}

>Answer: The game should not be played.

- Write a R code to simulate possible results for playing the games 500 times  

```{r}
par(mfrow=c(3,3))
plots=map(1:9,
          ~cumsum(x=sample(c(2,-1), # random walk
                  size=500, 
                  replace=T,
                  prob=c(0.3,0.7)))) %>% 
      map(
          plot,         # plot
          type="o", 
          col="blue",
          xlab="number of playing games",
          ylab="the accumulated money")
```

## Martingale

### Slide 50   {.unnumbered}

For each toss of a fair coin, you will win $\$1$ if it lands up heads and lose $\$1$ if it lands up tails. Suppose that you start the game with $\$0$ and denote $M_k$ is the amount of money you obtain after k tosses. Then $(M_k)_{k\in\mathbb{N}}$ is a random walk.  

- Compute the probability of winning after playing the game 9 times.  

> We need to find the probability that $M_9$ or the amount of money after nine
plays greater than 0

>$$\begin{align*}
P(M_9>0)&=P(5H4T)+P(6H3T)+P(7H2T)+P(8H1T)+P(9H0T) \\
&= \sum_{k=5}^{9} \binom{9}{k}p^k(1-p)^{9-k} \\
&=0.5
\end{align*}$$

> Answer: The probability of winning after playing the game 9 times is  50\%

- Compute the probability of winning after playing the game 10 times.

> We need to find the probability that $M_{10}$ or the amount of money after ten
plays greater than 0

>$$\begin{align*}
P(M_{10}>0)&=P(6H4T)+P(7H3T)+P(8H2T)+P(9H1T)+P(10H0T) \\
&= \sum_{k=6}^{10} \binom{10}{k}p^k(1-p)^{10-k} \\
&=0.3769
\end{align*}$$

> Answer: The probability of winning after playing the game 10 times is  37.69\%

- Is this game fair, i.e., the chance of winning a fair game is equal for all players?

>$$\begin{align*} 
E[X] &= E[X|X=1]P(X=1)+E[X|X=-1]P(X=-1) \\
&=\frac{1}{2}(1)+\frac{1}{2}(-1) \\
&=0
\end{align*}$$

>If the process $M_k$ is a martingale, it is a fair game.
$$\begin{align*}
E[M_{k+1} | \mathcal{F}_k] &= E[M_k+X_{k+1} | \mathcal{F}_k] \\
&=M_k+E[X_{k+1} | \mathcal{F}_k] \\
&=M_k
\end{align*}$$

> Answer: This is a fair game.

```{r}
par(mfrow=c(3,3))
plots=map(1:9,
          ~cumsum(x=sample(c(2,-1), # random walk
                  size=500, 
                  replace=T,
                  prob=c(0.3,0.7)))) %>% 
      map(
          plot,         # plot
          type="o", 
          col="blue",
          xlab="number of playing games",
          ylab="the accumulated money")
```



### Slide 55   {.unnumbered}

Consider a game based on a standard normal random variable Z. At each run, the player will receive the money equals to the value of Z at that run. Suppose that you start the game with $\$0$ and denote $M_k$ is the amount of money you obtain after k plays.  

- Given that $\displaystyle{\int_{\frac{1}{\sqrt{10}}}^{+\infty} \frac{1}{\sqrt{2\pi}}e^{-x^2/2} \, dx}=0.376$, find the probability that $M_{10} > 1$.  

>$$M_{10}=\sum_{i=1}^{10}Z_i \sim N(0,\sqrt{10})$$

>$$\begin{align*}
P(M_{10}>1)&=P(\frac{M_{10}}{\sqrt{10}}>\frac{1}{\sqrt{10}}) \\
&=P(Z>0.3162) \\
&=P(Z<-0.3162) \\
&=0.37591 
\end{align*}$$

> Answer: The probability that $M_{10} > 1$ is 36.591\%.

- Should you play the game, i.e., is this a fair game?  

>$$\begin{align*} 
    E[Z] &= E[Z|Z=1]P(Z=1)+E[Z|Z=-1]P(Z=-1) \\
    &=\frac{1}{2}(1)+\frac{1}{2}(-1) \\
    &=0
    \end{align*}$$
    
>$$\begin{align*}E[M_{k+1} | \mathcal{F}_k] &= E[M_k+Z_{k+1} | \mathcal{F}_k] \\
&=E[M_k| \mathcal{F}_k]+E[Z_{k+1} | \mathcal{F}_k] \\
&=M_k+0 \\
&=M_k
\end{align*}$$

> Answer: This is a fair game.

- Play the game 10 times and plot your accumulated money after every play. Write a R code to simulate the game  

```{r}
par(mfrow=c(3,3))
plots=map(1:9,
          ~cumsum(x=rnorm(10))) %>% 
      map(
          plot,         # plot
          type="o", 
          col="blue",
          xlab="number of playing games",
          ylab="the accumulated money")
```

### Slide 58   {.unnumbered}

Consider a game based on a standard normal random variable Z. At each run, the player will receive the money equals to $\$0.3$ plus the value of Z at that run. Suppose that you start the game with $\$0$ and denote $M_k$ is the amount of money you obtain after k plays.  

- Compute the probability that $M_{10}>0$, given that $\displaystyle{\int_0^{0.3\sqrt{10}}\frac{1}{\sqrt{2\pi}}e^{-x^2/2} \, dx}=0.3286$.

>$$M_{10}=\sum_{i=1}^{10}0.3+Z_i=3+\sum_{i=1}^{10}Z_i \sim N(3,\sqrt{10})$$

>$$\begin{align*}P(M_{10}>0) &= P(\frac{M_{10}-3}{\sqrt{10}}>\frac{0-3}{\sqrt{10}}) \\
&=P(Z>\frac{-3}{\sqrt{10}}) \\
&=P(Z<\frac{3}{\sqrt{10}}) \\ 
&=0.8286 
\end{align*}$$

> Answer: The probability that $M_{10} > 0$ is 82.86\%.

- Should you play the game, i.e., is this a fair game?  

>$$\begin{align*}
E[M_{k}|\mathcal{F}_{k-1}] &= E\left[\sum_{i=1}^{k}(0.3+Z_i) | \mathcal{F}_{k-1}\right] \\
&=\sum_{i=1}^{k}0.3+\sum_{i=1}^{k}E[Z_i| \mathcal{F}_{k-1}] \\
&=0.3k
\end{align*}$$

> Answer: This is not a fair game.

- Write a R code to simulate the obtained result after playing the game 500 times.  

```{r}
par(mfrow=c(3,3))
plots=map(1:9,
          ~cumsum(x=rnorm(500))) %>% 
      map(
          plot,         # plot
          type="o", 
          col="blue",
          xlab="number of playing games",
          ylab="the accumulated money")
```

### Slide 76 {.unnumbered}

Prove the following random processes are martingale:

1. Brownian motion $(B_t)$ is a martingale process.

Let $\{\mathcal{F}_t\}_{t \geq 0}$ be the natural filtration of Brownian motion $\{ B_t \}_{t \geq0}$. For all $0 \leq s <t$, we have:

>$$\begin{align*}
E[|B_t|]&= \int_{-\infty}^{\infty}|x|\frac{1}{\sqrt{2 \pi t}}e^{\frac{-x^2}{2t}}\,dx  \\
&=\int_{-\infty}^{0}|x|\frac{1}{\sqrt{2 \pi t}}e^{\frac{-x^2}{2t}}\,dx+\int_{0}^{\infty}|x|\frac{1}{\sqrt{2 \pi t}}e^{\frac{-x^2}{2t}}\,dx \\
&=\frac{1}{\sqrt{2 \pi t}}\int_{0}^{\infty}xe^{\frac{-x^2}{2t}}\,dx+\frac{1}{\sqrt{2 \pi t}}\int_{0}^{\infty}xe^{\frac{-x^2}{2t}}\,dx \\
&=\sqrt{\frac{2}{{\pi t}}}\int_{0}^{\infty}xe^{\frac{-x^2}{2t}}\,dx \\
&=\frac{1}{\sqrt{2 \pi t}}\int_{0}^{\infty}e^{\frac{-x^2}{2t}}\,dx^2 \\
&=-\sqrt{\frac{2t}{\pi}}e^{\frac{-u}{2t}}\Biggr|_{0}^{\infty} \\
&=\sqrt{\frac{2t}{\pi}} < \infty
\end{align*}$$

>$$\begin{align*}
E[B_t|\mathcal{F}_s]&=E[(B_t-B_s)+B_s|\mathcal{F}_s] \\
&=E[(B_t-B_s)|\mathcal{F}_s]+E[B_s|\mathcal{F}_s] \\ 
&=0+B_s \\
&=B_s 
\end{align*}$$

2. Let $Y_t = B_t^2 − t, \forall t \geq 0$. Show that $(Y_t)_{t \geq 0}$ is a martingale with respect to Brownian motion.

>Let $\{\mathcal{F}_t\}_{t \geq 0}$ be the natural filtration of $\{ Y_t=B_t^2-t \}_{t \geq0}$. For all $0 \leq s <t$, we have:

>$$\begin{align*}
E[|Y_t|]&=E[|B_t^2-t] \\
&\leq E[B_t^2+t] \\
&=E[B_t^2]+t \\
&=2t < \infty
\end{align*}$$

>$$\begin{align*}
E[Y_t|\mathcal{F}_s]&=E[B_t^2-t|\mathcal{F}_s] \\
&=E[(B_t-B_s+B_s)^2|\mathcal{F}_s]-t \\
&=E[(B_t-B_s)^2+2B_s(B_t-B_s)+B_s^2|\mathcal{F}_s]-t \\ 
&=E[(B_t-B_s)^2|\mathcal{F}_s]+E[2B_s(B_t-B_s)|\mathcal{F}_s]+E[B_s^2|\mathcal{F}_s]-t \\
&=E[(B_t-B_s)^2]+0+B_s^2-t \\
&=E[(B_t-B_s)^2]-E[B_t-B_s]^2+B_s^2-t \\
&=Var[B_t-B_s]+B_s^2-t \\
&=t-s+B_s^2-t \\
&=B_s^2-s
\end{align*}$$

3. $Z_t = e^{\sigma B_t − \frac{1}{2} \sigma^2 t}$ ($\sigma$ is a positive constant) is a martingale with respect to the standard Brownian motion.

>Let $\{\mathcal{F}_t\}_{t \geq 0}$ be the natural filtration of $\{ Z_t=e^{\sigma B_t − \frac{1}{2} \sigma^2 t}\}$. For all $0 \leq s <t$, since $B_t-B_s \sim N(0,t-s)$ we have:

>$$\begin{align*}
E[|Z_t|]&=E[|e^{\sigma B_t − \frac{1}{2} \sigma^2 t}|] \\
&=E[e^{\sigma B_t − \frac{1}{2} \sigma^2 t}] \\
&=E[Z_t] \\
&=1
\end{align*}$$

>$$E[e^{tX}]=e^{\mu t+\frac{1}{2}\sigma^2 t^2} \rightarrow E[e^{\sigma (B_t-B_s)}]=e^{\frac{1}{2}\sigma^2 (t-s)}$$

>$$\begin{align*}
E[Z_t|\mathcal{F}_s]&=E[e^{\sigma B_t − \frac{1}{2} \sigma^2 t}|\mathcal{F}_s] \\
&=E[e^{\sigma (B_t-B_s+B_s) − \frac{1}{2} \sigma^2 (t-s+s)}|\mathcal{F}_s] \\ 
&=e^{\sigma B_s-\frac{1}{2}\sigma_s^2} \cdot E[e^{\sigma (B_t-B_s)} \cdot e^ {− \frac{1}{2} \sigma^2 (t-s)}|\mathcal{F}_s] \\
&=e^{\sigma B_s-\frac{1}{2}\sigma_s^2} \cdot E[e^{\sigma (B_t-B_s)} \cdot e^ {− \frac{1}{2} \sigma^2 (t-s)}] \\
&=e^{\sigma B_s-\frac{1}{2}\sigma_s^2} \cdot E[e^{\sigma (B_t-B_s)}] \cdot E[e^ {− \frac{1}{2} \sigma^2 (t-s)}] \\
&=e^{\sigma B_s-\frac{1}{2}\sigma_s^2}  \\
&= Z_s
\end{align*}$$

## Brownian Motion

### Slide 65 {.unnumbered}

A particle’s position is modelled by a standard Brownian motion. Find the probability that the particle’s position at time t = 3 is above the level 1.5.

>Let $(B_t)_t \geq 0$ be the position of the particle at time t is $B_t$.
The position of the particle at time t = 3 is $B_3$.

>The desired probability is
$$\begin{align*}
P(B_3 \geq 1.5 | B_0 = 0)&=P(B_3 \geq 1.5) \\
&= 1-P(B_3 \leq 1.5) \\
&=1-P(Z \leq \frac{1.5}{\sqrt{3}}) \\
&=1-pnorm(1.5,0,sqrt(3))=`r 1-pnorm(1.5,0,sqrt(3))`
\end{align*}$$

>Answer: The probability that the particle’s position at time t = 3 is above the level 1.5 is `r round(1-pnorm(1.5,0,sqrt(3)),4)*100`\%

### Slide 65' {.unnumbered}

A particle’s position is modelled by a standard Brownian motion. Find the probability that the particle’s position at time t = 7 is above the level 2.

>Let $(B_t)_t \geq 0$ be the position of the particle at time t is $B_t$.
The position of the particle at time t = 7 is $B_3$.

>The desired probability is
$$\begin{align*}
P(B_7 \geq 2 | B_0 = 0)&=P(B_7 \geq 2) \\
&= 1-P(B_7 \leq 2) \\
&=1-P(Z \leq \frac{2}{\sqrt{7}}) \\
&=1-pnorm(2,0,sqrt(7))=`r 1-pnorm(2,0,sqrt(7))`
\end{align*}$$

>Answer: The probability that the particle’s position at time t = 3 is above the level 1.5 is `r round(1-pnorm(2,0,sqrt(7)),4)*100`\%

# Ito - Doeblin Formula

## Riemann Approach

### Slide 79 {.unnumbered}

Compute the Riemann integral $\int_0^1x^2\,dx=$  by using definition

>Let $\Pi$ be a partition of $[0, 1]$:
$$0 = t_0 < t_1 = \frac{1}{n} < t_2 = \frac{2}{n} < ... < t_i < ... < t_n = 1,t_i = \frac{i}{n}$$

>Since $f(x) = x^2$ is a continuous function of $[0, 1]$, the Riemann integral of $f(x)$ on $[0, 1]$ is
$$\begin{align*}
\int_0^1x^2\,dx &= \lim_{n\to\infty} \sum_{i=0}^{n-1}(t_{i+1}-t_i)f(t_i) \\
&=\lim_{n\to\infty} \sum_{i=0}^{n-1} \frac{1}{n} \left( \frac{i }{n}\right)^2 \\
&=\lim_{n\to\infty} \frac{1}{n^3} \sum_{i=0}^{n-1}  i^2 \\
&=\lim_{n\to\infty} \frac{(n-1)(n)(2n-1)}{6n^3} \\
&=\frac{1}{3}
\end{align*}$$ 

>Answer: $\int_0^1x^2\,dx=\frac{1}{3}$

### Slide 79' {.unnumbered}

Compute the Riemann integral $\int_0^2x^2\,dx=$  by using definition

>Let $\Pi$ be a partition of $[0, 2]$:
$$0 = t_0 < t_1 = \frac{2}{n} < t_2 = \frac{4}{n} < ... < t_i < ... < t_n = 2,t_i = \frac{2i}{n}$$

>Since $f(x) = x^2$ is a continuous function of $[0, 2]$, the Riemann integral of $f(x)$ on $[0, 2]$ is
$$\begin{align*}
\int_0^2x^2\,dx &= \lim_{n\to\infty} \sum_{i=0}^{n-1}(t_{i+1}-t_i)f(t_i) \\
&=\lim_{n\to\infty} \sum_{i=0}^{n-1} \frac{2}{n} \left( \frac{2i }{n}\right)^2 \\
&=\lim_{n\to\infty} \frac{8}{n^3} \sum_{i=0}^{n-1}  i^2 \\
&=\lim_{n\to\infty} \frac{8(n-1)(n)(2n-1)}{6n^3} \\
&=\frac{8}{3}
\end{align*}$$ 

>Answer: $\int_0^2x^2\,dx=\frac{8}{3}$

### Slide 86 {.unnumbered}

1. Compute the following Ito’s integral

$$\int_5^9 \,dX_t$$

>Let $t_i=5+i\cdot \frac{9-5}{n}=5+ \frac{4i}{n}$ then 
$$\begin{align*}
&t_0=5+ \frac{4 \cdot 0}{n}=5 \\
&t_n=5+ \frac{4 \cdot n}{n}=9
\end{align*}$$

>$$\begin{align*}
\int_5^9 \,dX_t&=\lim_{n\to\infty}  \sum_{i=0}^{n-1} 1 \cdot(X_{t_{i+1}}-X_{t_{i}}) \\
&=\lim_{n\to\infty} (X_{t_1}-X_{t_0})+(X_{t_2}-X_{t_1})+...+(X_{t_n}-X_{t_{n-1}}) \\
&= X_9 -X_5
\end{align*}$$

>Answer: $\int_5^9 \,dX_t=X_9 -X_5$

2. Compute the following Ito’s integral

$$\int_2^t \,dX_s^2$$

>Let $t_i=2+i\cdot \frac{t-2}{n}=2+ \frac{(t-2)i}{n}$ then 
$$\begin{align*}
&t_0=2+ \frac{(t-2) \cdot 0}{n}=2 \\
&t_n=2+ \frac{(t-2) \cdot n}{n}=t
\end{align*}$$

>$$\begin{align*}
\int_2^t \,dX_s&=\lim_{n\to\infty}  \sum_{i=0}^{n-1} 1 \cdot(X_{t_{i+1}}^2-X_{t_{i}}^2) \\
&=\lim_{n\to\infty} (X_{t_1}^2-X_{t_0}^2)+(X_{t_2}^2-X_{t_1}^2)+...+(X_{t_n}^2-X_{t_{n-1}}^2) \\
&= X_t^2 -X_2^2
\end{align*}$$

>Answer: $\int_5^9 \,dX_t=X_t^2 -X_2^2$

### Slide 86 (1) {.unnumbered}

Compute the following Ito’s integral

$$\int_7^{10}\,dX_t^3$$

>Let $t_i=7+i\cdot \frac{10-7}{n}=7+ \frac{3i}{n}$ then 
$$\begin{align*}
&t_0=7+ \frac{3 \cdot 0}{n}=7 \\
&t_n=7+ \frac{3 \cdot n}{n}10
\end{align*}$$

>$$\begin{align*}
\int_5^9 \,dX_t&=\lim_{n\to\infty}  \sum_{i=0}^{n-1} 1 \cdot(X_{t_{i+1}}^3-X_{t_{i}}^3) \\
&=\lim_{n\to\infty} (X_{t_1}^3-X_{t_0}^3)+(X_{t_2}^3-X_{t_1}^3)+...+(X_{t_n}^3-X_{t_{n-1}}^3) \\
&= X_{10}^3 -X_7^3
\end{align*}$$

>Answer: $\int_7^{10} \,dX_t^3=X_{10}^3 -X_7^3$

## Differential Form

### Slide 94   {.unnumbered}

Find the differential form of the following stochastic processes:  

1. $M_t=\displaystyle\int_0^t3W_s^2\,ds$  

>$$dM_t = 3W_t^2\,dt$$  

2. $M_t=\displaystyle\int_0^t3W_s\,dWs$  

>$$dM_t = 3W_t\,dW_t$$  

3. $X_t=3+\displaystyle\int_0^t3s\,dW_s+\displaystyle\int_0^t5W_s^2\,ds$  

>$$dX_t = 3t\,dW_t + 5W_t^2\,dt$$  

## Integral Form

### Slide 95   {.unnumbered}

Find the integral form of the following stochastic processes:  

1. $dX_t = 5\,dt + 3\,dW_t$  

>$$\begin{align*}
X_t &= X_0 + \int_0^t5\,ds + \int_0^t3\,dW_s \\
&= X_0+5t+3W_t
\end{align*}$$

>Answer: $X_t=X_0+5t+3W_t$

2. $dX_t = 4X_t\,dt + 3X_t\,dW_t$  

>Let $Y_t=\ln X_t$ and $f(t,x)=\ln x$, we have:
$$\begin{align*}
f_t&=0 \\
f_x&=\frac{1}{x} \\
f_{tt}&=0 \\
f_{xx}&=-\frac{1}{x^2} \\
f_{tx}&=0 \\
\end{align*}$$

>Applying Ito Doeblin formula, we have:

>$$\begin{align*}
\,d(Y_t)&=\frac{1}{X_t} \,dX_t-\frac{1}{2}\frac{1}{X_t^2}\,dX_t^2 \\
&=\frac{1}{X_t} (4X_t\,dt + 3X_t\,dW_t)-\frac{1}{2}\frac{1}{X_t^2}(4X_t\,dt + 3X_t\,dW_t)^2 \\
&=\frac{1}{X_t}\cdot4X_t\,dt + \frac{1}{X_t} \cdot 3X_t\,dW_t-\frac{1}{2}\frac{1}{X_t^2} \cdot 9X_t^2\,dW_t^2 \\
&=4\,dt + 3\,dW_t-\frac{9}{2}\,dt \\
\,d \ln X_t&=-\frac{1}{2}\,dt + 3\,dW_t \\
\int_0^t \,d \ln X_s&= \int_0^t-\frac{1}{2}\,ds +\int_0^t 3\,dW_s \\
\ln X_s\Biggr|_{0}^{t} &= -\frac{1}{2}\Biggr|_{0}^{t}+3W_s\Biggr|_{0}^{t} \\
\ln \left( \frac{X_t}{X_0} \right) &= -\frac{1}{2}t+3W_t \\
X_t &= X_0e^{-\frac{1}{2}t+3W_t}
\end{align*}$$

>Answer: $X_t = X_0e^{-\frac{1}{2}t+3W_t}$

3. $\,d W_t^3=3W_t \,dt+3W_t^2 \,dW_t$  

>$$\begin{align*}
W_t^3 &= W_0 + \int_0^t 3 W_s \,ds + \int_0^t 3W_s^2 \,dW_s \\
&=
\end{align*}$$

### Slide 96   {.unnumbered}

Find the integral form form of the following stochastic processes:

- Arithmetic Brownian motion: $dX_t = 5\,dt + 3\,dW_t$  

>$$X_t = X_0 + \int_0^t5\,ds + \int_0^t3\,dW_s$$

- Geometric Brownian motion: $dX_t = 5X_t\,dt + 5X_t\,dW_t$ 

>$$X_t = X_0 + \int_0^t5X_s \,ds + \int_0^t5X_s\,dW_s$$

## Taylor Expansion

### Slide 97   {.unnumbered}

For one-variable functions, their second-order Taylor series expansions have the form  
$$df(x)=f'(x)\,dx+\frac{1}{2}f''(x)\,dx^2$$  

- Find the second-order Taylor series expansion of $f(x)=e^x$  

><p>We have $df(x) = f'(x)\,dx + \frac{1}{2}f''(x)\,dx^2$. Therefore, the second-order Taylor series will be  
$$f(x) = f(x_0) + f'(x_0)(x-x_0) + \frac{1}{2}f''(x_0)(x-x_0)^2$$</p>

><p>Let $f(x)=e^x$ and $x_0=0$, thus  
$$\begin{align*}
f(x) &= e^0 + e^0x + \frac{1}{2}e^0(x)^2 \\
&= \frac{1}{2}x^2 + x + 1
\end{align*}$$</p>

- Find the second-order Taylor series expansion of $f(x)=\sin x$  

><p>We have $df(x) = f'(x)\,dx + \frac{1}{2}f''(x)\,dx^2$. Therefore, the second-order Taylor series will be  
$$f(x) = f(x_0) + f'(x_0)(x-x_0) + \frac{1}{2}f''(x_0)(x-x_0)^2$$</p>

><p>Let $f(x)=\sin (x)$ and $x_0=0$, thus  
$$\begin{align*}
f(x) &= \sin (0) + x \cos (0) - \frac{1}{2} \sin (x)^2 \\
&= x
\end{align*}$$</p>

- Find the second-order Taylor series expansion of $f(x)=\ln x,\,x>0$  

><p>We have $df(x) = f'(x)\,dx + \frac{1}{2}f''(x)\,dx^2$. Therefore, the second-order Taylor series will be  
$$f(x) = f(x_0) + f'(x_0)(x-x_0) + \frac{1}{2}f''(x_0)(x-x_0)^2$$</p>

><p>Let $f(x)=\ln x$ and $x_0=1$, thus  
$$\begin{align*}
f(x) &= \ln (1) + x-1 - \frac{1}{2}(x-1)^2 \\
&= x-1 - \frac{1}{2}(x-1)^2 \\
&= -\frac{1}{2}x^2+2x-\frac{3}{2}
\end{align*}$$</p>

### Slide 100 {.unnumbered}

1. Find the second-order Taylor series expansion of $f(x)=e^{x+y}$  

> We have  

$$\begin{align*}
f_x&=e^{x+y} \\
f_{xx}&=e^{x+y} \\ 
f _{xy}&=e^{x+y} \\ 
f_y&=e^{x+y} \\
f_{yy}&=e^{x+y}
\end{align*}$$ 

> Apply the Taylor series expansions we have
\begin{align*}
df(x,y) &= f_x(x,y)\,dx + f_y(x,y)\,dy + \frac{1}{2}f_{x^2}\,dx^2 + \frac{1}{2}f_{y^2}\,dy^2 + f_{xy}\,dxdy \\  
&= e^{x+y}\,dx + e^{x+y}\,dy + \frac{1}{2}e^{x+y}\,dx^2 + \frac{1}{2}e^{x+y}\,dy^2 + e^{x+y}\,dxdy \\
&= e^{x+y}(\,dx +\,dy + \frac{1}{2}\,dx^2 + \frac{1}{2}\,dy^2 + \,dxdy)
\end{align*}

>Answer: The second-order Taylor series expansion of $f(x)$ is 
$$e^{x+y}(\,dx +\,dy + \frac{1}{2}\,dx^2 + \frac{1}{2}\,dy^2 + \,dxdy)$$

2. Find the second-order Taylor series expansion of $f(x) = \sin(xy)$ 

> We have
$$\begin{align*}
f_x&=y \cos (xy) \\ 
f_{xx}&=-y^2 \sin (xy) \\ 
f _{xy}&= \cos (xy) - xy \sin (xy) \\
f_y&=x \cos (xy) \\ 
f_{yy}&=-x^2 \sin (xy) &&
\end{align*}$$

>Apply the Taylor series expansions, we havee
$$\begin{align*}
df(x,y) &= f_x(x,y)\,dx + f_y(x,y)\,dy + \frac{1}{2}f_{x^2}\,dx^2 + \frac{1}{2}f_{y^2}\,dy^2 + f_{xy}\,dxdy \\
&= y \cos (xy)\,dx + x \cos (xy)\,dy + \frac{1}{2}y^2 \sin (xy)\,dx^2 + \frac{1}{2}x^2 \sin (xy)\,dy^2 + \cos (xy) - xy \sin (xy)\,dxdy
\end{align*}$$

>Answer: The second-order Taylor series expansion of $f(x)$ is 
$$y \cos (xy)\,dx + x \cos (xy)\,dy + \frac{1}{2}y^2 \sin (xy)\,dx^2 + \frac{1}{2}x^2 \sin (xy)\,dy^2 + \cos (xy) - xy \sin (xy)\,dxdy$$

3. Find the second-order Taylor series expansion of $f(x)=\ln \left( \frac{x}{y} \right),x,y >0$  

> We have
$$\begin{align*}
f_x&=\frac{1}{x} \\ 
f_{xx}&=-\frac{1}{x^2} \\ 
f _{xy}&=0 \\
f_y&=-\frac{1}{y} \\ 
f_{yy}&=\frac{1}{y^2}
\end{align*}$$

> Apply the formula as
\begin{align*}
df(x,y) &= f_x(x,y)\,dx + f_y(x,y)\,dy + \frac{1}{2}f_{x^2}\,dx^2 + \frac{1}{2}f_{y^2}\,dy^2 + f_{xy}\,dxdy \\
&= \frac{1}{x}\,dx - \frac{1}{y}\,dy + \frac{1}{x^2}\,dx^2 + \frac{1}{y^2}\,dy^2
\end{align*}

>Answer: The second-order Taylor series expansion of $f(x)$ is $\frac{1}{x}\,dx - \frac{1}{y}\,dy + \frac{1}{x^2}\,dx^2 + \frac{1}{y^2}\,dy^2$

## $\,dt$ and $\,dW_t$

### Slide 99' {.unnumbered}

3. Let $\,dX_t = 7\,dt + 3\,dW_t$. Compute $\,d(\sin X_t)$ in terms of $\,dt$ and $\,dW_t$.  

>Let $f(t,x)=\sin x$ then
$$\begin{align*}
&f_t =0 \\ 
&f_{tt} =0 \\
&f_x =\cos x \\ 
&f_{xx} =-\sin x \\
&f_{tx} =0 \\
\end{align*}$$

>Applying Ito formula to $f(X_t) = \sin X_t$, we have:
$$\begin{align*}
\,d(sinX_t) &= cosX_t\,dX_t - \frac{1}{2}sinX_t\,d X_t^2 \\  
&= \cos X_t(7\,dt + 3\,dW_t) - \frac{1}{2} \sin X_t(7\,dt + 3\,dW_t)^2 \\  
&= \cos X_t \cdot 7\,dt + \cos X_t \cdot 3\,dW_t - \frac{1}{2} \sin X_t \cdot 9\,dt \\  
&= (7 \cos X_t - \frac{9}{2} \sin X_t)\,dt + 3 \cos X_t\,dW_t
\end{align*}$$

>Answer: $d(\sin X_t)=(7 \cos X_t - \frac{9}{2} \sin X_t)\,dt + 3 \cos X_t\,dW_t$


4. Let $\,d X_t =7 \,dt+3 \,dW_t$. Compute $\,d(X_t^2)$ in terms of $\,dt$ and $\,dW_t$.  

>Let $f(t,x)=x^2$ then
$$\begin{align*}
&f_t =0 \\ 
&f_{tt} =0 \\
&f_x = 2x \\ 
&f_{xx} =2 \\
&f_{tx} =0 \\
\end{align*}$$

>Applying Ito formula to $f(X_t) =  X_t^2$, we have:  
$$\begin{align*}
d(X_t^2) &= 2X_t \,dX_t + 2\,dX_t^2 \\  
&= 2X_t (7 \,dt+3 \,dW_t) + 2(7 \,dt+3 \,dW_t)^2 \\
&= 14X_t \,dt+6 X_t \,dW_t + 2\cdot 9 \,dt \\
&= (14X_t+9)\,dt + 6X_t\,dW_t
\end{align*}$$

>Answer: $d(X_t^2)=(14X_t+9)\,dt + 6X_t\,dW_t$

### Slide 103' {.unnumbered}

>Let $(W_t)_{t \geq 0}$ be a Brownian motion on the filtered probability space $(\omega,\mathcal{F},(\mathcal{F_t})_{t \geq 0},\mathcal{P})$. Consider a process $(X_t)_{t \geq 0}: X_t = e^{2 W_t -2t}$ . Compute $\,d X_t$ in terms of $\,d W_t$ and $\,dt$.

>$f(t,x)=e^{2 x -7t}$ then
$$\begin{align*}
&f_t =-7 e^{2 x -7t} \\ 
&f_{tt} =49 e^{2 x -7t} \\
&f_x = 2e^{2 x -7t} \\ 
&f_{xx} =4e^{2 x -7t} \\
&f_{tx} =-14 e^{2 x -7t} \\
\end{align*}$$

>Applying Ito formula to $f(W_t) =  e^{2 W_t -7t}$, we have:  

>$$\begin{align*}
d(e^{2 W_t -7t}) &= -7 e^{2 W_t -7t}\,dt +49 e^{2 W_t -7t}\,dt^2 +2e^{2 W_t -7t}\,dW_t + 4e^{2 W_t -7t}\,d W_t^2 -14 e^{2 W_t -7t}\,dW_t\,dt \\
&= -7 e^{2 W_t -7t}\,dt +0 +2e^{2 W_t -7t}\,dW_t + 4e^{2 W_t -7t}\,d t -0 \\
&= -3 e^{2 W_t -7t}\,dt +2e^{2 W_t -7t}\,dW_t\\
\end{align*}$$

>Answer: $d(e^{2 W_t -7t})=-3 e^{2 W_t -7t}\,dt +2e^{2 W_t -7t}\,dW_t$


### Slide 99 {.unnumbered}

1. Compute $\,dW_t^2$ in terms of $\,dt$ and $\,dW_t$.  

> Apply the formula  
$$df(t,W_t) = f_t(t,W_t)\,dt + f_{w_t}(t,W_t)\,dW_t + \frac{1}{2}f_{w_t^2}(t,W_t)\,dt$$  

> Thus, we have  
\begin{align*}
d(W_t^2) &= 0\,dt + 2W_t\,dW_t + \frac{1}{2}\cdot2\,dt \\
&= \,dt+2W_t\,dW_t
\end{align*}

>Answer: $d(W_t^2)=2W_t\,dW_t + \,dt$

2. Compute $\,d(\sin W_t)$ in terms of $\,dt$ and $\,dW_t$.  

> Apply the formula  
$$df(t,W_t) = f_t(t,W_t)\,dt + f_{w_t}(t,W_t)\,dW_t + \frac{1}{2}f_{w_t^2}(t,W_t)\,dt$$  

> Thus, we have  
\begin{align*}
d(sinW_t) &= 0\,dt + cosW_t\,dW_t + \frac{1}{2}(-sinW_t)\,dt \\
&= - \frac{1}{2}sinW_t\,dt + cosW_t\,dW_t 
\end{align*}

>Answer: $d(sinW_t)=cosW_t\,dW_t - \frac{1}{2}sinW_t\,dt$

3. Let $\,dX_t = 5\,dt + 3\,dW_t$. Compute $\,d(\sin X_t)$ in terms of $\,dt$ and $\,dW_t$.  
>Let $f(t,x)=\sin x$ then
$$\begin{align*}
&f_t =0 \\ 
&f_{tt} =0 \\
&f_x =\cos x \\ 
&f_{xx} =-\sin x \\
&f_{tx} =0 \\
\end{align*}$$

>Applying Ito formula to $f(X_t) = \sin X_t$, we have:
$$\begin{align*}
\,d(sinX_t) &= cosX_t\,dX_t - \frac{1}{2}sinX_t\,d X_t^2 \\  
&= cosX_t(5\,dt + 3\,dW_t) - \frac{1}{2}sinX_t(5\,dt + 3\,dW_t)^2 \\  
&= cosX_t \cdot 5\,dt + cosX_t \cdot 3\,dW_t - \frac{1}{2}sinX_t \cdot 9\,dt \\  
&= (5cosX_t - \frac{9}{2}sinX_t)\,dt + 3cosX_t\,dW_t
\end{align*}$$

>Answer: $d(sinX_t)=(5cosX_t - \frac{9}{2}sinX_t)\,dt + 3cosX_t\,dW_t$


4. Let $\,d X_t =5 \,dt+3 \,dW_t$. Compute $\,d(X_t^2)$ in terms of $\,dt$ and $\,dW_t$.  

>Let $f(t,x)=x^2$ then
$$\begin{align*}
&f_t =0 \\ 
&f_{tt} =0 \\
&f_x = 2x \\ 
&f_{xx} =2 \\
&f_{tx} =0 \\
\end{align*}$$

>Applying Ito formula to $f(X_t) =  X_t^2$, we have:  
$$\begin{align*}
d(X_t^2) &= 2X_t \,dX_t + 2\,dX_t^2 \\  
&= 2X_t (5 \,dt+3 \,dW_t) + 2(5 \,dt+3 \,dW_t)^2 \\
&= 10X_t \,dt+6 X_t \,dW_t + 2\cdot 9 \,dt \\
&= (10X_t+9)\,dt + 6X_t\,dW_t
\end{align*}$$

>Answer: $d(X_t^2)=(10X_t+9)\,dt + 6X_t\,dW_t$

5. Let $\,d X_t =5 \,dt+3 \,dW_t$. Compute $\,d(e^{X_t})$ in terms of $\,dt$ and $\,d W_t$.  

> Apply the formula  
$$df(X_t) = f'(X_t)\,dX_t + \frac{1}{2}f''(X_t)\,dX_t^2$$  

> Thus, we have  

>$$\begin{align*}
df(X_t) &= e^{X_t}\,dX_t + \frac{1}{2}e^{X_t}\,dX_t^2 \\
&= e^{X_t}(5 \,dt+3 \,dW_t) + \frac{1}{2}e^{X_t}(5 \,dt+3 \,dW_t)^2 \\
&= 5e^{X_t} \,dt+3e^{X_t} \,dW_t + \frac{1}{2}e^{X_t} \cdot9 \,dt \\
&= \frac{19}{2}e^{X_t}\,dt + 3e^{X_t}\,dW_t  
\end{align*}$$

>Answer: $d(e^{X_t})=\frac{19}{2}e^{X_t}\,dt + 3e^{X_t}\,dW_t$

6. Let $\,d X_t =5 X_t \,dt+3 X_t \,dW_t$. Compute $\,d(X_t^2)$ in terms of $\,dt$ and $\,dW_t$.  

> Apply the formula  
$$df(X_t) = f'(X_t)\,dX_t + \frac{1}{2}f''(X_t)\Delta_t^2\,dt$$  

> Thus, we have  
\begin{align*}
d(X_t^2) &= 2X_t\,dX_t + \frac{1}{2} \cdot 2 (5 X_t \,dt+3 X_t \,dW_t)^2 \\
d(X_t^2) &= 2X_t(5 X_t \,dt+3 X_t \,dW_t) + 9 X_t^2 \,dt \\
d(X_t^2) &= 10X_t^2\,dt+6 X_t^2 \,dW_t + 9 X_t^2 \,dt \\
&= 19X_t^2\,dt + 6X_t^2\,dW_t
\end{align*}

>Answer: $d(X_t^2)=19X_t^2\,dt + 6X_t^2\,dW_t

7. Let $\,d X_t =5 X_t \,dt+3 X_t \,dW_t$. Compute $\,d(\sin X_t)$ in terms of $\,dt$ and $\,dW_t$.  

> Apply the formula
$$df(X_t) = f'(X_t)\,d X_t + \frac{1}{2}f''(X_t)\,d X_t^2$$

> Thus, we have
$$\begin{align*}
d(\sin X_t)&= \cos X_t \,d X_t -\frac{1}{2} \sin X_t \,d X_t^2 \\
&= \cos X_t (5 X_t \,dt+3 X_t \,dW_t) -\frac{1}{2} \sin X_t (5 X_t \,dt+3 X_t \,dW_t)^2 \\
&=  5 X_t \cos X_t \,dt+3X_t \cos X_t \,dW_t -\frac{1}{2} \sin X_t \cdot 9 X_t^2 \,dt \\
&=  (5 X_t \cos X_t -\frac{9}{2} X_t^2 \sin X_t) \,dt +3X_t \cos  X_t \,dW_t  \\
\end{align*}$$

>Answer: $d(sinX_t)=(5 X_t \cos X_t -\frac{9}{2} X_t^2 \sin X_t) \,dt +3X_t \cos  X_t \,dW_t$

8. Let $\,d X_t =5 X_t \,dt+3 X_t \,dW_t$. Compute $\,d(e^{X_t})$ in terms of $\,dt$ and $\,d W_t$.  

> Apply the formula
$$df(X_t) = f'(X_t)\,dX_t + \frac{1}{2}f''(X_t)\,d X_t^2$$

> Thus, we have  
$$\begin{align*}
df(X_t) &= e^{X_t}\,dX_t + \frac{1}{2}e^{X_t}\,dX_t^2 \\
&= e^{X_t}(5 X_t \,dt+3 X_t \,dW_t) + \frac{1}{2}e^{X_t}(5 X_t \,dt+3 X_t \,dW_t)^2 \\
&= 5e^{X_t} X_t \,dt+3X_te^{X_t} \,dW_t + \frac{1}{2}e^{X_t} \cdot9 X_t^2 \,dt \\
&= \left(5X_t + \frac{9}{2} X_t^2 \right)e^{X_t}\,dt + 3 X_t e^{X_t}\,dW_t  
\end{align*}$$

>Answer: $d(e^{X_t})=\frac{19}{2}e^{X_t}\,dt + 3e^{X_t}\,dW_t$

9. Let $\,d X_t =5 X_t \,dt+3 X_t \,dW_t$. Compute $\,d( \ln {X_t})$ in terms of $\,dt$ and $\,d W_t$.  

> Apply the formula
$$df(X_t) = f'(X_t)\,dX_t + \frac{1}{2}f''(X_t)\,d X_t^2$$

>

$$\begin{align*}
d(\ln X_t) &= \frac{1}{X_t}\,dX_t + \frac{1}{2} \left( -\frac{1}{X_t^2} \right)\,d X_t^2 \\
&= \frac{1}{X_t}(5X_t\,dt + 3X_t\,dW_t) - \frac{1}{2X_t^2}(5X_t\,dt + 3X_t\,dW_t)^2 \\
&= \frac{1}{X_t} \cdot 5X_t\,dt + \frac{1}{X_t} \cdot 3X_t\,dW_t - \frac{1}{2X_t^2} \cdot 9X_t^2\,dt \\
&= \frac{1}{2}\,dt + 3\,dW_t 
\end{align*}$$

>Answer: $d(\ln X_t)=3\,dW_t + \frac{1}{2}\,dt$

10. Let $\,d X_t = 5(4 − X_t) \,dt + 3X_t \,dW_t$. Compute $\,d(e^{5t}X_t)$ in terms of $\,dt$ and $\,d W_t$. 
>With 

$f(t,x)=e^{5t}x$ then we can calculate:
$$\begin{align*} 
f_t&=5e^{5t}x \\
f_x&=e^{5t} \\
f_{tt}&=5^2e^{5t}x \\
f_{xx}&=0 \\
f_{tx}&=5e^{5t}
\end{align*}$$

> Apply the Ito Doeblin formula, we have
$$\begin{align*}
d(e^{5t}X_t) &=5e^{5t}X_t \,dt +  e^{5t} \,dX_t + 5^2e^{5t}X_t \,dt^2 +0 \,dX_t^2+5e^{5t}\,dt \,dX_t \\
&=5e^{5t}X_t \,dt +  e^{5t} (5(4 − X_t) \,dt + 3X_t \,dW_t) + 5e^{5t}\,dt (5(4 − X_t) \,dt + 3X_t \,dW_t) \\
&=5e^{5t}X_t \,dt +  5(4 − X_t)e^{5t} \,dt + 3X_te^{5t} \,dW_t + 5e^{5t} 5(4 − X_t) \,dt^2 + 3\cdot5 X_t \\
&=20e^{5t} \,dt + 3X_te^{5t} \,dW_t
\end{align*}$$

>Answer: $d(e^{5t}X_t)=(20e^{5t}-5X_te^{5t}+\frac{9}{2}e^{5t}X_t^2)\,dt + 3e^{5t}X_t\,dW_t$

### Slide 104 {.unnumbered}

Let $(W_t)_{t \geq 0}$ be a Brownian motion on the filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_t)_{t \geq 0},\mathcal{P})$. Compute $\,dX_t$ in terms of $\,dW_t$ and $\,dt$, where:

1. $X_t=10e^{0.5 \left(0.1-\frac{0.3^2}{6} \right)t+\frac{0.3}{\sqrt{3} W_t}}$.  

> We have
\begin{align*}
f_t=0.425f(t,W_t) \qquad & f_{W_tW_t}= 3f(t,W_t) \\
f_{W_t}=\sqrt{3}f(t,W_t) \qquad &  
\end{align*}

> Apply the formula as
\begin{align*}
df(t,W_t) &= f_t(t,W_t)\,dt &&+ f_{W_t}(t,W_t)\,dW_t +&&& \frac{1}{2}f_{W_t^2}(t,W_t)\,dW_t^2 \\ 
&= 0.425X_t\,dt &&+ \sqrt{3}X_t\,dW_t +&&& \frac{3}{2}X_t\,dW_t^2 \\ 
&= 0.425X_t\,dt &&+ \sqrt{3}X_t\,dW_t +&&& \frac{3}{2}X_t\,dt \\
&= 1.925X_t\,dt &&+ \sqrt{3}X_t\,dW_t &&&
\end{align*}

>Answer: $df(t,W_t)=1.925X_t\,dt+ \sqrt{3}X_t\,dW_t$

2. $X_t=Y_t^3$, with $\,d Y_t=2Y_t\,dt+3Y_t\,dW_t$.  

> Let $f(t,Y_t) = X_t = Y_t^3$, we can calculate $f_t=0 \\ f_{Y_t}=3Y_t^2\\ f_{Y_tY_t}=6Y_t$

> Apply the formula as
\begin{align*}
d(X_t) = df(t,Y_t) = f_t(t,Y_t)\,dt + f_{Y_t}(t,Y_t)\,dY_t + \frac{1}{2}f_{Y_tY_t}(t,Y_t)\,dY_t\,dY_t
\end{align*}

> We got \ $d(X_t) = 3Y_t^2\,dY_t + 3Y_t\,dY_t\,dY_t$

> Since \ $dY_t = 2Y_t\,dt + 3Y_t\,dW_t\\ dY_t\,dY_t = (3Y_t)^2\,dt$

> We have
\begin{align*}
\,d(X_t) &= 3Y_t^2\,(2Y_t\,dt + 3Y_t\,dW_t) + 3Y_t . 9Y_t^2\,dt\\
&= 33Y_t^3\,dt + 9Y_t^3\,dW_t
\end{align*}

>Answer: $\,d(X_t)=33Y_t^3\,dt + 9Y_t^3\,dW_t$

3. $X_t=\ln Y_t$, with $\,dY_t=4tY_t\,dt+2tY_t\,dW_t$.  

> Let $f(t,Y_t) = X_t = \ln Y_t$, we can calculate \ $f_t=0 \\ f_{Y_t}=\frac{1}{Y_t}\\ f_{Y_tY_t}=-\frac{1}{Y_t^2}$

> Apply the formula as
\begin{align*}
d(X_t) = df(t,Y_t) = f_t(t,Y_t)\,dt + f_{Y_t}(t,Y_t)\,dY_t + \frac{1}{2}f_{Y_tY_t}(t,Y_t)\,dY_t\,dY_t
\end{align*}

> We got \ $d(X_t) = \frac{1}{Y_t}\,dY_t - \frac{1}{2Y_t^2}\,dY_t\,dY_t$

> Since \ $dY_t = 4tY_t\,dt + 2tY_t\,dW_t\\ dY_t\,dY_t = (2tY_t)^2\,dt$

> We have
\begin{align*}
d(X_t) &= \frac{1}{Y_t}\,(4tY_t\,dt + 2tY_t\,dW_t) + \frac{1}{2Y_t^2} . (2tY_t^2)\,dt\\
&= (4t-2t^2)\,dt + 2t\,dW_t
\end{align*}

>Answer: $\,d(X_t)=(4t-2t^2)\,dt + 2t\,dW_t$

4. $X_t=\frac{4-Y_t}{1-t}$, with $\,dY_t=X_t\,dt+\,dW_t$.

>$$ X_t=\frac{4-Y_t}{1-t} \rightarrow Y_t=X_t(t-1)+4 $$

>$$\begin{align*}
f(t,x)=\frac{4-x}{1-t}=\frac{x-4}{t-1}
\end{align*}$$

>$$\begin{align*}
&f_t =-\frac{x-4}{(t-1)^2} \\ 
&f_{tt} =\frac{2(x-4)}{(t-1)^3}\\
&f_x =\frac{1}{t-1} \\ 
&f_{xx} =0 \\
&f_{tx} =-\frac{1}{(t-1)^2} \\
\end{align*}$$

>$$\begin{align*}
\,dX_t&=-\frac{Y_t-4}{(t-1)^2}\,dt + \frac{1}{t-1} \,dY_t + \frac{1}{2}\frac{2(Y_t-4)}{(t-1)^3} \,dt^2 -\frac{1}{(t-1)^2} \,dW_t \,dt \\ 
&= -\frac{Y_t-4}{(t-1)^2}\,dt + \frac{1}{t-1} \,dY_t \\
&= -\frac{X_t(t-1)+4-4}{(t-1)^2}\,dt + \frac{1}{t-1} (X_t\,dt+\,dW_t) \\
&= \frac{1}{t-1} \,dW_t
\end{align*}$$

>Answer: $\,dX_t=\frac{1}{t-1} \,dW_t$

## Differential Equation

### Slide 106 {.unnumbered}

1. $ty′(t)+2y(t)=4t^2, \quad y(1)=2$.  

> Standard form  
$$y'(t) + \frac{2}{t}y(t) = 4t$$  

> The integrating factor will be  
$$\mu(t)=e^{\int{\frac{2}{t}}\,dt}=t^2$$  

> The general solution will be given by  
$$y=\frac{1}{t^2} \left(\:\int4t^3\,dt+C\:\right)
=\frac{1}{t^2} \left(t^4+C\right)$$

> Since $y(1)=2$, we have that  
$$1(1+C) = 2 \quad \text{implying } \quad C = 1 $$  

> Therefore, $\qquad y=\frac{1}{t^2}\left(t^4+1\right)$  

>Answer: $y=\frac{1}{t^2}\left(t^4+1\right)$

2. $2y′(t)+ty(t)=2,\quad y(0)=1$.  

> Standard form  
$$y'(t) + \frac{t}{2}y(t) = 1$$  

> The integrating factor will be  
$$\mu(t)=e^{\int{\frac{t}{2}}\,dt}=e^{\frac{t^2}{4}}$$  

> We have that  
\begin{align*}
\frac{d}{dx} &\left(e^{\frac{t^2}{4}}y\right) &&= e^{\frac{t^2}{4}} \\ 
\rightarrow \quad &e^{\frac{t^2}{4}}y &&= \displaystyle\int e^{\frac{t^2}{4}}\,dt + C \\
\rightarrow \quad &y &&= \frac{1}{e^{\frac{t^2}{4}}}\displaystyle\int e^{\frac{t^2}{4}}\,dt + C
\end{align*}

> We have $y(0)=1, \quad \text{implying } \; C = 0$  

> Therefore, $\qquad y=\frac{1}{e^{\frac{t^2}{4}}}\displaystyle\int e^{\frac{t^2}{4}}\,dt$  

>Answer: $y=\frac{1}{e^{\frac{t^2}{4}}}\displaystyle\int e^{\frac{t^2}{4}}\,dt$

3. $y′(t)−y(t)=2te^{2t},y(0)=1$. 

> The integrating factor will be
$$\mu(t)=e^{\int-1\,dt}=e^{-t}$$  

> The general solution will be given by 
$$y=e^t \left(\: \int2te^t\,dt + C \:\right)
=e^t\left[\,(2+e^t-2e^t) + C \,\right]$$

> Since $y(0)=1$, we have that  
$$1[(0-2)+C] = 1 \quad \text{implying } \quad C = 3 $$  

> Therefore, $\qquad y=e^t\left[\,(2+e^t-2e^t) + 3 \,\right]$ 

>Answer: $y=e^t\left[\,(2+e^t-2e^t) + 3 \,\right]$

4. $y′+y^2 \sin x=0,y(2)=1$.  

>We have that  
\begin{align*}
y' + y^2sinx &= 0 \\
\Leftrightarrow \frac{dy}{dx} + y^2sinx &= 0 \\
\Leftrightarrow \frac{1}{y^2}\,dy + sinx\,dx &= 0
\end{align*}

> $$\left.\begin{align*}
\text{Let: } \quad &M(x,y) = \sin x \\ &N(x,y) = \frac{1}{y^2}  
\end{align*}\right\}  
\rightarrow \frac{\partial M}{\partial y}=\frac{\partial N}{\partial x}=0 
\quad \text{: the equation is exact}
$$  

> So there exists $F(x,y)$, such that  
$$\frac{\partial F}{\partial x}=M(x,y)=\sin(x) \quad(1) \quad\text{ and }
\quad \frac{\partial F}{\partial y}=N(x,y)=\frac{1}{y^2} \quad(2)$$  

>   
Equation (1) implies that  
\begin{align*}F(x,y)&=\int\sin x\,dx=-\cos x + \varphi(y) \\
\text{Thus, } \quad &\frac{\partial F}{\partial y}=\varphi'(y) \qquad (3)
\end{align*}  
Equation (2) and (3) gives  
\begin{align*}
\frac{1}{y^2} &= \varphi'(y) \\  
\rightarrow \varphi(y) &= -\frac{1}{y} \\  
\end{align*}  

> 
\begin{align*}
F(x,y) &= -\cos x -\frac{1}{y} = C \\
\rightarrow y(x) &= \frac{1}{-\cos x - C} \\
\iff 1 &= \frac{1}{-\cos(2) - C} \qquad \text{ since } \; y(2)=1 \\ 
\text{or } \quad C &= -1 -\cos(2) \\ 
\text{Thus, } \quad y &= \frac{1}{-\cos x - 1 - \cos(2)}
\end{align*}

>Answer: y = $\frac{1}{-\cos x - 1 - \cos(2)}$

5. $y′ =(1−2x)y^2,y(0)=1$.  

> We have that  
\begin{align*}
y' &= (1-2x)y^2 \\
\Leftrightarrow \frac{\,dy}{\,dx} &= (1-2x)y^2 \\
\Leftrightarrow (1-2x)\,dx - \frac{1}{y^2}\,dy &= 0
\end{align*}

> $$\left.\begin{align*}
\text{Let: } \quad &M(x,y) = 1-2x \\ &N(x,y) = -\frac{1}{y^2}  
\end{align*}\right\}  
\rightarrow \frac{\partial M}{\partial y}=\frac{\partial N}{\partial x}=0 
\quad \text{: the equation is exact}
$$  

> So there exists $F(x,y)$, such that  
$$\frac{\partial F}{\partial x}=M(x,y)=1-2x \quad(1) \quad\text{ and }
\quad \frac{\partial F}{\partial y}=N(x,y)=-\frac{1}{y^2} \quad(2)$$  

>  
Equation (1) implies that  
\begin{align*}F(x,y)&=\int(1-2x)\,dx= x-x^2 + \varphi(y) \\
\text{Thus, } \quad &\frac{\partial F}{\partial y}=\varphi'(y) \qquad (3)
\end{align*}  
Equation (2) and (3) gives  
\begin{align*}
\varphi'(y) &= -\frac{1}{y^2} \\  
\rightarrow \varphi(y) &= \frac{1}{y} \\  
\end{align*}  

> 
\begin{align*}
F(x,y) &= x-x^2+\frac{1}{y} = C \\
\rightarrow y(x) &= \frac{1}{-x+x^2+C} \\
\iff 1 &= \frac{1}{C} \qquad \text{ since } \; y(0)=1 \\ 
\text{or } \quad C &= 1 \\ 
\text{Thus, } \quad y &= \frac{1}{x^2-x+1}
\end{align*}

>Answer: $y = \frac{1}{x^2-x+1}$

6. $x \,dx+ye^{−x}\,dy=0,y(0)=1$  

> 
$$\left.\begin{align*}
\text{Let: } \quad &M(x,y) = 1-2x& N(x,y) &= -\frac{1}{y^2} \\
\quad &\frac{\partial M}{\partial y} =0& \frac{\partial N}{\partial x}&=-ye^{-x} 
\end{align*}\right\}  
\rightarrow \frac{\partial M}{\partial y}\neq\frac{\partial N}{\partial x} 
\quad \text{: not exact}
$$  

> Moreover,
\begin{align*}
\frac{\frac{\partial M}{\partial y}-\frac{\partial N}{\partial x}}{N}
&=\frac{0+ye^{-x}}{ye^{-x}} = 1 \\ 
\text{thus, } \quad \mu(x) &= e^{\int\,dx} =e^x \quad \text{is an integrating factor of the equation} \\ 
\rightarrow e^xx\,dx + y\,dy &= 0 \qquad 
\text{(by multiplying both side with $\mu(x)=1$)} \\ 
\rightarrow \frac{\partial M}{\partial y}=\frac{\partial N}{\partial x} &=0
\quad \text{: exact} 
\end{align*}

> So there exists $F(x,y)$, such that  
$$\frac{\partial F}{\partial x}=M(x,y)=xe^x \quad(1) \quad\text{ and }
\quad \frac{\partial F}{\partial y}=N(x,y)= y \quad(2)$$  

>    
Equation (2) implies that  
\begin{align*}F(x,y)&=\int y \,dy= \frac{1}{2}y^2 + \varphi(x) \\
\text{Thus, } \quad &\frac{\partial F}{\partial x}=\varphi'(x) \qquad (3)
\end{align*}  
Equation (1) and (3) gives  
\begin{align*}
\varphi'(x) &= xe^x \\  
\rightarrow \varphi(x) &= xe^x-e^x \\  
\end{align*}  

> 
\begin{align*}
F(x,y) &= \frac{1}{2}y^2 + xe^x - e^x = C \\
\rightarrow y(x) &= \sqrt{2(C+e^x-xe^x)} \\
\iff 1 &= \sqrt{2(C+1-0)} \qquad \text{ since } \; y(0)=1 \\ 
\text{or } \quad C &= -\frac{1}{2} \\ 
\text{Thus, } \quad y &= \sqrt{2(-\frac{1}{2}+e^x-xe^x)}
\end{align*}

>Answer: $y = \sqrt{2(-\frac{1}{2}+e^x-xe^x)}$

7. $y'(x)=\frac{x^2+xy+y^2}{x^2},y(2)=2$.  

>With $u=y+\frac{x}{2}$ and $v=\frac{2}{\sqrt{3}x}u$ 
$$\int \frac{1}{x^2+xy+y^2} \,dy \\
=\int \frac{1}{\left(y+\frac{x}{2} \right)^2+\frac{3}{4}x^2} \,dy \\
=\int \frac{1}{\left(u \right)^2+\frac{3}{4}x^2} \,du \\
=\int \frac{4}{4u^2+3x^2} \,du \\
=\int \frac{4}{3x^2v^2+3x^2} \frac{x\sqrt{3}}{2}\,dv \\
=\frac{4}{2\sqrt{3}x}\int \frac{1}{v^2+1}\,dv \\
=\frac{4}{2\sqrt{3}x}\arctan(v) +c \\
=\frac{2}{\sqrt{3}x} \arctan(\frac{2}{\sqrt{3}x}y+\frac{1}{\sqrt{3}})+c
$$

>$$\begin{align*}
&y'(x)=\frac{x^2+xy+y^2}{x^2} \\
&\rightarrow \frac{\,dy}{\,dx}=\frac{x^2+xy+y^2}{x^2} \\
&\rightarrow \frac{1}{x^2+xy+y^2} \,dy=\frac{1}{x^2}\,dx \\
&\rightarrow \frac{2}{\sqrt{3}x} \arctan(\frac{2}{\sqrt{3}x}y+\frac{1}{\sqrt{3}})+c=-\frac{1}{x} \\
&\rightarrow y=\frac{x\sqrt{3}}{2} \tan \left( cx-\frac{\sqrt{3}}{2} \right)-\frac{x}{2}
\end{align*}$$

>$$\begin{align*}
&y(2)=2 \\
&\rightarrow c=\frac{\pi}{6}+\frac{\sqrt{3}}{4}+\frac{k\pi}{2} \\
&\rightarrow y=\frac{x\sqrt{3}}{2} \tan \left( \left( \frac{\pi}{6}+\frac{\sqrt{3}}{4}+\frac{k\pi}{2}  \right)x-\frac{\sqrt{3}}{2} \right)-\frac{x}{2}  
\end{align*}$$

>Answer: $y=\frac{x\sqrt{3}}{2} \tan \left( \left( \frac{\pi}{6}+\frac{\sqrt{3}}{4}+\frac{k\pi}{2}  \right)x-\frac{\sqrt{3}}{2} \right)-\frac{x}{2}$

8. $y'(x)=\frac{x^2+3y^2}{2xy},y(2)=2$.  

> We have  
\begin{align*}
y'(x)&=\frac{x^2+3y^2}{2xy} \\
(x^2+3y^2)\,dx - (2xy)\,dy &= 0
\end{align*}

>    
$$\left.\begin{align*}
\text{Let: } \quad &M(x,y) = x^2+3y^2& N(x,y) &= -2xy \\
\quad &\frac{\partial M}{\partial y} =6y& \frac{\partial N}{\partial x}&=-2y 
\end{align*}\right\}  
\rightarrow \frac{\partial M}{\partial y}\neq\frac{\partial N}{\partial x} 
\quad \text{: not exact}
$$  

> Moreover,
\begin{align*}
\frac{\frac{\partial M}{\partial y}-\frac{\partial N}{\partial x}}{N}
&=\frac{6y+2y}{-2xy} = \frac{8y}{-2xy}=\frac{-4}{x} \\ 
\text{thus, } \quad \mu(x) &= e^{\int\frac{-4}{x}\,dx} =\frac{1}{x^4} \quad \text{is an integrating factor} \\
\rightarrow \left(\frac{1}{x^2}+\frac{3y^2}{x^4}\right)\,dx &- \left(\frac{2y}{x^3}\right)\,dy = 0 \quad \text{(by multiplying both side with } \mu(x)=\frac{1}{x^4} \text{)} \\
\rightarrow \frac{\partial M}{\partial y}=\frac{\partial N}{\partial x} &=\frac{6y}{x^4}
\quad \text{: exact} 
\end{align*}

> So there exists $F(x,y)$, such that  
$$\frac{\partial F}{\partial x}=M(x,y)=\frac{1}{x^2} + \frac{3y^2}{x^4} \quad(1) \quad\text{ and }
\quad \frac{\partial F}{\partial y}=N(x,y)= \frac{-2y}{x^3} \quad(2)$$  

>    
Equation (2) implies that  
\begin{align*}F(x,y)&=\int\frac{-2y}{x^3}\,dy= -\frac{y^2}{x^3} + \varphi(x) \\
\text{Thus, } \quad &\frac{\partial F}{\partial x}=\frac{3y^2}{x^4}+\varphi'(x) \qquad (3)
\end{align*}  
Equation (1) and (3) gives  
\begin{align*}
\frac{3y^2}{x^4}+\varphi'(x) &= \frac{3y^2}{x^4}+\frac{1}{x^2} \\  
\rightarrow \varphi'(x) &= \frac{1}{x^2} \\  
\rightarrow \varphi(x) &= -\frac{1}{x}
\end{align*}  

> 
\begin{align*}
F(x,y) &= -\frac{y^2}{x^3} -\frac{1}{x} = C \\
\rightarrow y(x) &= \sqrt{-x^3C-x^2} \\
\iff 2 &= \sqrt{-(2)^3C-4} \qquad \text{ since } \; y(2)=2 \\ 
\text{or } \quad C &= -1 \\ 
\text{Thus, } \quad y &= \sqrt{x^3-x^2}
\end{align*}

>Answer: $y = \sqrt{x^3-x^2}$

## Geometric Brownian

### Slide 118 {.unnumbered}

Let $\{(W_t)_t \geq 0\}$ be a standard Brownian motion on the probability space $(\Omega,\mathcal{F},\mathcal{P})$. Suppose that the process $(X_t)_t \geq 0$ is governed by the geometric Brownian motion:
$\,d X_t = 8X_t\,dt + 2X_t\,dW_t$.
Find the distribution, the mean and variance of the random variable $(X_5|X_3 = 4)$.

>Applying Ito formula to $Y_t = \ln X_t$ with $f(t,x)= \ln x$, we have:
$$f_t= 0 , f_x= \frac{1}{X_t}, f_{xx}=  -\frac{1}{X_t^2}$$
then 
$$\begin{align*}
dY_t&= \frac{1}{X_t}dX_t - \frac{1}{2}\frac{1}{X_t^2}dX_tdX_t \\
&= \frac{1}{X_t}(8X_tdt + 2X_tdW_t) - \frac{1}{2}\frac{1}{X_t^2}(8X_tdt + 2X_tdW_t)^2 \\
&= 6dt+2dW_t \\
\rightarrow \int^5_3 dY_t &=\int^5_3 6dt +\int^5_3 2dW_t \\
\rightarrow \ln X_5 - \ln X_3 &=  (5-3) \cdot 6 + 2 \cdot (W_5-W_3) \\
\rightarrow X_5&= e^{\ln X_3 + 12 + 2(W_5-W_3)}
\end{align*}$$

>$$\begin{align*}
\rightarrow \ln X_3+12+2(W_5-W_3) &\sim \mathcal{N} \left(\ln X_3+12,2 \cdot 2 \right) \\
\rightarrow e^{\ln X_2+\frac{584}{5}+\int_2^4 t^2dW_t} &\sim \log\mathcal{N} \left(\ln X_3+12,2 \cdot 2 \right) \\
\rightarrow X_5 &\sim \log\mathcal{N} \left(\ln X_3+12,2 \cdot 2 \right) \\
\end{align*}$$

>$$\begin{align*}
\rightarrow (X_5|X_3=4) &\sim \log\mathcal{N} \left(\ln 4+12,4 \right) \\
\end{align*}$$

>$$\begin{align*}
E[X_5|X_3=4]&=e^{\ln 4+12+ \frac{1}{2} \cdot 4} \\
&=e^{15.38} \\
Var(X_5|X_3=4)&=\left(e^{4}-1\right)e^{\left(2(\ln 4+12)+4 \right)}  \\
&=(e^{4}-1)e^{30.77}
\end{align*}$$

>The mean and variance of $(X_5|X_3=4)$ are $e^{15.38}$ and $(e^{4}-1)e^{30.77}$

### Slide 122 {.unnumbered}

Let $\{(W_t)_t \geq 0\}$ be a standard Brownian motion on the probability space $(\Omega,\mathcal{F},\mathcal{P})$. Suppose that the process $(X_t)_t \geq 0$ is governed by the geometric Brownian motion:
$\,d X_t =4(t^3−3)X_t\,dt+t^2X_t\,dW_t$.
Find the distribution, the mean and variance of the random variable
$(X_4|X_2 = 3)$.

>Applying Ito formula to $Y_t = \ln X_t$ with $f(t,x)= \ln x$, we have:
$$f_t= 0 , f_x= \frac{1}{X_t}, f_{xx}=  -\frac{1}{X_t^2}$$
then $$dY_t= \frac{1}{X_t}\,dX_t - \frac{1}{2}\frac{1}{X_t^2}\,dX_t\,dX_t$$

>$$\begin{align*}
dY_t&= \frac{1}{X_t}(4(t^3−3)X_t\,dt+t^2X_t\,dW_t) - \frac{1}{2}\frac{1}{X_t^2}(4(t^3−3)X_t\,dt+t^2X_t\,dW_t)^2 \\
&= \left( -\frac{1}{2}t^4+4t^3-12 \right) \,dt+t^2dW_t \\
\rightarrow \int_2^4 \,dY_t  &= \int_2^4 \left( -\frac{1}{2}t^4+4t^3-12 \right) \,dt+\int_2^4t^2dW_t \\
\rightarrow  Y_4-Y_2  &= \frac{584}{5}+\int_2^4 t^2dW_t \\
\rightarrow  \ln X_4-\ln X_2  &= \frac{584}{5}+\int_2^4 t^2dW_t \\
\rightarrow  X_4  &= e^{\ln X_2+\frac{584}{5}+\int_2^4 t^2dW_t} \\
\end{align*}$$

>$$\begin{align*}
\int_2^4 t^2 \,d W_t &\sim \mathcal{N} \left(0,\int_2^4 t^4 \,d t \right) \\
&\sim \mathcal{N}\left(0,\frac{992}{5} \right) \\
\end{align*}$$

>$$\begin{align*}
\rightarrow \ln X_2+\frac{584}{5}+\int_2^4 t^2dW_t &\sim \mathcal{N} \left(\ln X_2+\frac{584}{5},\frac{992}{5} \right) \\
\rightarrow e^{\ln X_2+\frac{584}{5}+\int_2^4 t^2dW_t} &\sim \log\mathcal{N} \left(\ln X_2+\frac{584}{5},\frac{992}{5} \right) \\
\rightarrow X_4 &\sim \log\mathcal{N} \left(\ln X_2+\frac{584}{5},\frac{992}{5} \right) \\
\end{align*}$$

>$$\begin{align*}
(X_4|X_2=3) &\sim \log\mathcal{N} \left(\ln 3+\frac{584}{5},\frac{992}{5} \right) \\
\end{align*}$$

>$$\begin{align*}
E[X_4|X_2=3]&=e^{\ln 3+\frac{584}{5}+ \frac{1}{2} \cdot \frac{992}{5}} \\
&=e^{217.09} \\
Var(X_4|X_2=3)&=\left(e^{\frac{992}{5}}-1\right)e^{\left(2(\ln 3+\frac{584}{5})+\frac{992}{5} \right)}  \\
&=(e^{198.4}-1)e^{434.19}
\end{align*}$$

>Answer: The mean and variance of $(X_4|X_2=3)$ are $e^{217.09}$ and $(e^{198.4}-1)e^{434.19}$